#YamlMime:PythonReference
api_name: []
items:
- _type: module
  children:
  - cntk.train.distributed.Communicator
  - cntk.train.distributed.DistributedLearner
  - cntk.train.distributed.WorkerDescriptor
  - cntk.train.distributed.block_momentum_distributed_learner
  - cntk.train.distributed.data_parallel_distributed_learner
  fullName: cntk.train.distributed
  langs:
  - python
  module: cntk.train.distributed
  name: distributed
  source:
    id: distributed
    path: cntk/train/distributed.py
    remote:
      branch: master
      path: cntk/train/distributed.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 0
  summary: Distributed learners manage learners in distributed environment.
  type: Namespace
  uid: cntk.train.distributed
- _type: function
  fullName: cntk.train.distributed.block_momentum_distributed_learner
  langs:
  - python
  module: cntk.train.distributed
  name: block_momentum_distributed_learner
  seealso: 'See also: [1] K. Chen and Q. Huo. [Scalable training of deep learning
    machines by incremental block training with intra-block parallel optimization
    and blockwise model-update filtering](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/0005880.pdf).
    Proceedings of ICASSP, 2016.'
  source:
    id: block_momentum_distributed_learner
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 141
  summary: 'Creates a block momentum distributed learner. See [1] for more information.

    Block Momentum divides the full dataset into M non-overlapping blocks, and each
    block is partitioned into N non-overlapping splits.

    During training, a random, unprocessed block is randomly taken by the trainer
    and the N partitions of this block are dispatched on the workers.'
  syntax:
    content: block_momentum_distributed_learner()
    parameters:
    - description: a local learner (i.e. sgd)
      id: learner
    - description: size of the partition in samples
      id: block_size
      type:
      - int
    - description: block momentum as time constant
      id: block_momentum_as_time_constant
      type:
      - float
    - description: use nestrov momentum
      id: use_nestrov_momentum
      type:
      - bool
    - description: reset SGD momentum after aggregation
      id: reset_sgd_momentum_after_aggregation
      type:
      - bool
    - description: block learning rate
      id: block_learning_rate
      type:
      - float
    - description: number of samples after which distributed training starts
      id: distributed_after
      type:
      - int
    return:
      description: a distributed learner instance
  type: Method
  uid: cntk.train.distributed.block_momentum_distributed_learner
- _type: function
  fullName: cntk.train.distributed.data_parallel_distributed_learner
  langs:
  - python
  module: cntk.train.distributed
  name: data_parallel_distributed_learner
  source:
    id: data_parallel_distributed_learner
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 115
  summary: Creates a data parallel distributed learner
  syntax:
    content: data_parallel_distributed_learner()
    parameters:
    - description: a local learner (i.e. sgd)
      id: learner
    - description: number of samples after which distributed training starts
      id: distributed_after
      type:
      - int
    - description: number of bits for quantization (1 to 32)
      id: num_quantization_bits
      type:
      - int
    - description: use async buffered parameter update
      id: use_async_buffered_parameter_update
      type:
      - bool
    return:
      description: a distributed learner instance
  type: Method
  uid: cntk.train.distributed.data_parallel_distributed_learner
references:
- fullName: cntk.train.distributed.Communicator
  isExternal: false
  name: Communicator
  parent: cntk.train.distributed
  uid: cntk.train.distributed.Communicator
- fullName: cntk.train.distributed.DistributedLearner
  isExternal: false
  name: DistributedLearner
  parent: cntk.train.distributed
  uid: cntk.train.distributed.DistributedLearner
- fullName: cntk.train.distributed.WorkerDescriptor
  isExternal: false
  name: WorkerDescriptor
  parent: cntk.train.distributed
  uid: cntk.train.distributed.WorkerDescriptor
- fullName: cntk.train.distributed.block_momentum_distributed_learner
  isExternal: false
  name: block_momentum_distributed_learner
  parent: cntk.train.distributed
  uid: cntk.train.distributed.block_momentum_distributed_learner
- fullName: cntk.train.distributed.data_parallel_distributed_learner
  isExternal: false
  name: data_parallel_distributed_learner
  parent: cntk.train.distributed
  uid: cntk.train.distributed.data_parallel_distributed_learner
