#YamlMime:PythonReference
api_name: []
items:
- _type: module
  children:
  - cntk.ops.abs
  - cntk.ops.alias
  - cntk.ops.argmax
  - cntk.ops.argmin
  - cntk.ops.as_block
  - cntk.ops.as_composite
  - cntk.ops.associative_multi_arg
  - cntk.ops.batch_normalization
  - cntk.ops.ceil
  - cntk.ops.clip
  - cntk.ops.combine
  - cntk.ops.constant
  - cntk.ops.convolution
  - cntk.ops.convolution_transpose
  - cntk.ops.cos
  - cntk.ops.dropout
  - cntk.ops.element_divide
  - cntk.ops.element_max
  - cntk.ops.element_min
  - cntk.ops.element_select
  - cntk.ops.element_times
  - cntk.ops.elu
  - cntk.ops.equal
  - cntk.ops.exp
  - cntk.ops.floor
  - cntk.ops.forward_backward
  - cntk.ops.future_value
  - cntk.ops.greater
  - cntk.ops.greater_equal
  - cntk.ops.hardmax
  - cntk.ops.input_variable
  - cntk.ops.labels_to_graph
  - cntk.ops.leaky_relu
  - cntk.ops.less
  - cntk.ops.less_equal
  - cntk.ops.log
  - cntk.ops.log_add_exp
  - cntk.ops.minus
  - cntk.ops.negate
  - cntk.ops.not_equal
  - cntk.ops.optimized_rnnstack
  - cntk.ops.output_variable
  - cntk.ops.param_relu
  - cntk.ops.parameter
  - cntk.ops.past_value
  - cntk.ops.per_dim_mean_variance_normalize
  - cntk.ops.placeholder_variable
  - cntk.ops.plus
  - cntk.ops.pooling
  - cntk.ops.random_sample
  - cntk.ops.random_sample_inclusion_frequency
  - cntk.ops.reciprocal
  - cntk.ops.reduce_log_sum_exp
  - cntk.ops.reduce_max
  - cntk.ops.reduce_mean
  - cntk.ops.reduce_min
  - cntk.ops.reduce_prod
  - cntk.ops.reduce_sum
  - cntk.ops.relu
  - cntk.ops.reshape
  - cntk.ops.roipooling
  - cntk.ops.round
  - cntk.ops.sigmoid
  - cntk.ops.sin
  - cntk.ops.slice
  - cntk.ops.softmax
  - cntk.ops.softplus
  - cntk.ops.splice
  - cntk.ops.sqrt
  - cntk.ops.square
  - cntk.ops.stop_gradient
  - cntk.ops.tanh
  - cntk.ops.times
  - cntk.ops.times_transpose
  - cntk.ops.transpose
  - cntk.ops.unpooling
  fullName: cntk.ops
  langs:
  - python
  module: cntk.ops
  name: ops
  source:
    id: ops
    path: cntk/ops/__init__.py
    remote:
      branch: master
      path: cntk/ops/__init__.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 0
  type: Namespace
  uid: cntk.ops
- _type: function
  fullName: cntk.ops.abs
  langs:
  - python
  module: cntk.ops
  name: abs
  source:
    id: abs
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1559
  summary: 'Computes the element-wise absolute of `x`:


    -[ Example ]-

    >>> C.abs([-1, 1, -2, 3]).eval()

    array([ 1.,  1.,  2.,  3.], dtype=float32)'
  syntax:
    content: abs()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.abs
- _type: function
  fullName: cntk.ops.alias
  langs:
  - python
  module: cntk.ops
  name: alias
  source:
    id: alias
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 108
  summary: Create a new Function instance which just aliases the specified 'x' Function/Variable
    such that the 'Output' of the new 'Function' is same as the 'Output' of the specified
    'x' Function/Variable, and has the newly specified name. The purpose of this operator
    is to create a new distinct reference to a symbolic computation which is different
    from the original Function/Variable that it aliases and can be used for e.g. to
    substitute a specific instance of the aliased Function/Variable in the computation
    graph instead of substituting all usages of the aliased Function/Variable.
  syntax:
    content: alias()
    parameters:
    - description: The Function/Variable to alias
      id: operand
    - description: the name of the Alias Function in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.alias
- _type: function
  fullName: cntk.ops.argmax
  langs:
  - python
  module: cntk.ops
  name: argmax
  source:
    id: argmax
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2301
  summary: "Computes the argmax of the input tensor's elements across the specified\
    \ axis. If no axis is specified, it will return the flatten index of the largest\
    \ element in tensor x.\n-[ Example ]-\n>>> # create 3x2 matrix in a sequence of\
    \ length 1 in a batch of one sample\n>>> data = [[10, 20],[30, 40],[50, 60]]\n\
    >>> C.argmax(data, 0).eval()\narray([[ 2.,  2.]], dtype=float32)\n>>> C.argmax(data,\
    \ 1).eval()\narray([[ 1.],\n       [ 1.],\n       [ 1.]], dtype=float32)"
  syntax:
    content: argmax()
    parameters:
    - description: any @cntk.ops.functions.Function that outputs a tensor.
      id: x
      type:
      - numpy.array or Function
    - description: axis along which the reduction will be performed
      id: axis
      type:
      - int or Axis
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, default to ''
    return:
      description: An instance of @cntk.ops.functions.Function
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.ops.argmax
- _type: function
  fullName: cntk.ops.argmin
  langs:
  - python
  module: cntk.ops
  name: argmin
  source:
    id: argmin
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2334
  summary: "Computes the argmin of the input tensor's elements across the specified\
    \ axis. If no axis is specified, it will return the flatten index of the smallest\
    \ element in tensor x.\n-[ Example ]-\n>>> # create 3x2 matrix in a sequence of\
    \ length 1 in a batch of one sample\n>>> data = [[10, 30],[40, 20],[60, 50]]\n\
    >>> C.argmin(data, 0).eval()\narray([[ 0.,  1.]], dtype=float32)\n>>> C.argmin(data,\
    \ 1).eval()\narray([[ 0.],\n       [ 1.],\n       [ 1.]], dtype=float32)"
  syntax:
    content: argmin()
    parameters:
    - description: any @cntk.ops.functions.Function that outputs a tensor.
      id: x
      type:
      - numpy.array or Function
    - description: axis along which the reduction will be performed
      id: axis
      type:
      - int or Axis
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, default to ''
    return:
      description: An instance of @cntk.ops.functions.Function
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.ops.argmin
- _type: function
  fullName: cntk.ops.as_block
  langs:
  - python
  module: cntk.ops
  name: as_block
  source:
    id: as_block
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 65
  summary: Create a new block Function instance which just encapsulates the specified
    composite Function to create a new Function that appears to be a primitive. All
    the arguments of the composite being encapsulated must be Placeholder variables.
    The purpose of block Functions is to enable creation of hierarchical Function
    graphs where details of implementing certain building block operations can be
    encapsulated away such that the actual structure of the block's implementation
    is not inlined into the parent graph where the block is used, and instead the
    block just appears as an opaque primitive. Users still have the ability to peek
    at the underlying Function graph that implements the actual block Function.
  syntax:
    content: as_block()
    parameters:
    - description: The composite Function that the block encapsulates
      id: composite
    - description: A list of tuples, mapping from block's underlying composite's arguments
        to
      id: block_arguments_map
    - description: ''
      id: variables they are connected to
      type:
      - actual
    - description: Name of the op that the block represents
      id: block_op_name
    - description: the name of the block Function in the network
      id: block_instance_name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.as_block
- _type: function
  fullName: cntk.ops.as_composite
  langs:
  - python
  module: cntk.ops
  name: as_composite
  source:
    id: as_composite
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 91
  summary: Creates a composite Function that has the specified root_function as its
    root. The composite denotes a higher-level Function encapsulating the entire graph
    of Functions underlying the specified rootFunction.
  syntax:
    content: as_composite()
    parameters:
    - description: Root Function, the graph underlying which, the newly created composite
        encapsulates
      id: root_function
    - description: the name of the Alias Function in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.as_composite
- _type: function
  fullName: cntk.ops.associative_multi_arg
  langs:
  - python
  module: cntk.ops
  name: associative_multi_arg
  source:
    id: associative_multi_arg
    path: cntk/ops/__init__.py
    remote:
      branch: master
      path: cntk/ops/__init__.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 638
  summary: 'The output of this operation is the result of an operation (*plus*, *log_add_exp*,
    *element_times*, *element_max*, *element_min*) of two or more input tensors. Broadcasting
    is supported.

    -[ Example ]-

    >>> C.plus([1, 2, 3], [4, 5, 6]).eval()

    array([ 5.,  7.,  9.], dtype=float32)

    >>> C.element_times([5., 10., 15., 30.], [2.]).eval()

    array([ 10.,  20.,  30.,  60.], dtype=float32)

    >>> C.plus([-5, -4, -3, -2, -1], [10], [3, 2, 3, 2, 3], [-13], [+42], ''multi_arg_example'').eval()

    array([ 37.,  37.,  39.,  39.,  41.], dtype=float32)

    >>> C.element_times([5., 10., 15., 30.], [2.], [1., 2., 1., 2.]).eval()

    array([  10.,   40.,   30.,  120.], dtype=float32)

    >>> a = np.arange(3,dtype=np.float32)

    >>> np.exp(C.log_add_exp(np.log(1+a), np.log(1+a*a)).eval())

    array([ 2.,  4.,  8.], dtype=float32)'
  syntax:
    content: associative_multi_arg(f)
    parameters:
    - description: left side tensor
      id: left
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.associative_multi_arg
- _type: function
  fullName: cntk.ops.batch_normalization
  langs:
  - python
  module: cntk.ops
  name: batch_normalization
  source:
    id: batch_normalization
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 427
  summary: Normalizes layer outputs for every minibatch for each output (feature)
    independently and applies affine transformation to preserve representation of
    the layer.
  syntax:
    content: batch_normalization()
    parameters:
    - description: input of the batch normalization operation
      id: operand
    - description: parameter tensor that holds the learned componentwise-scaling factors
      id: scale
    - description: parameter tensor that holds the learned bias. `scale` and `bias`
        must have the same dimensions which must be equal to the input dimensions
        in case of `spatial` = False or number of output convolution feature maps
        in case of `spatial` = True
      id: bias
    - description: running mean which is used during evaluation phase and might be
        used during training as well. You must pass a constant tensor with initial
        value 0 and the same dimensions as `scale` and `bias`
      id: running_mean
    - description: running variance. Represented as `running_mean`
      id: running_inv_std
    - description: Denotes the total number of samples that have been used so far
        to compute the `running_mean` and `running_inv_std` parameters. You must pass
        a scalar (either rank-0 `constant(val)`).
      id: running_count
    - description: flag that indicates whether to compute mean/var for each feature
        in a minibatch independently or, in case of convolutional layers, per future
        map
      id: spatial
      type:
      - bool
    - description: time constant for computing running average of mean and variance
        as a low-pass filtered version of the batch statistics.
      id: normalization_time_constant
      type:
      - float, default 5000
    - description: constant for smoothing batch estimates with the running statistics
      id: blend_time_constant
      type:
      - float, default 0
    - description: conditioner constant added to the variance when computing the inverse
        standard deviation
      id: epsilon
    - description: ''
      id: use_cudnn_engine
      type:
      - bool, default True
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.batch_normalization
- _type: function
  fullName: cntk.ops.ceil
  langs:
  - python
  module: cntk.ops
  name: ceil
  source:
    id: ceil
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1068
  summary: "The output of this operation is the element wise value rounded to the\
    \ smallest integer greater than or equal to the input.\n-[ Example ]-\n>>> C.ceil([0.2,\
    \ 1.3, 4., 5.5, 0.0]).eval()\narray([ 1.,  2.,  4.,  6.,  0.], dtype=float32)\n\
    >>> C.ceil([[0.6, 3.3], [1.9, 5.6]]).eval()\narray([[ 1.,  4.],\n       [ 2.,\
    \  6.]], dtype=float32)"
  syntax:
    content: ceil()
    parameters:
    - description: input tensor
      id: arg
    - description: the name of the Function instance in the network (optional)
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.ceil
- _type: function
  fullName: cntk.ops.clip
  langs:
  - python
  module: cntk.ops
  name: clip
  source:
    id: clip
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1132
  summary: 'Computes a tensor with all of its values clipped to fall between `min_value`
    and `max_value`, i.e. `min(max(x, min_value), max_value)`.

    The output tensor has the same shape as `x`.

    -[ Example ]-

    >>> C.clip([1., 2.1, 3.0, 4.1], 2., 4.).eval()

    array([ 2. ,  2.1,  3. ,  4. ], dtype=float32)

    >>> C.clip([-10., -5., 0., 5., 10.], [-5., -4., 0., 3., 5.], [5., 4., 1., 4.,
    9.]).eval()

    array([-5., -4.,  0.,  4.,  9.], dtype=float32)'
  syntax:
    content: clip()
    parameters:
    - description: tensor to be clipped
      id: x
    - description: a scalar or a tensor which represents the minimum value to clip
        element values to
      id: min_value
      type:
      - float
    - description: a scalar or a tensor which represents the maximum value to clip
        element values to
      id: max_value
      type:
      - float
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.clip
- _type: function
  fullName: cntk.ops.combine
  langs:
  - python
  module: cntk.ops
  name: combine
  source:
    id: combine
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 28
  summary: "Create a new Function instance which just combines the outputs of the\
    \ specified list of 'operands' Functions such that the 'Outputs' of the new 'Function'\
    \ are union of the 'Outputs' of each of the specified 'operands' Functions. E.g.,\
    \ when creating a classification model, typically the CrossEntropy loss Function\
    \ and the ClassificationError Function comprise the two roots of the computation\
    \ graph which can be combined to create a single Function with 2 outputs; viz.\
    \ CrossEntropy loss and ClassificationError output.\n-[ Example ]-\n>>> in1 =\
    \ C.input_variable((4,))\n>>> in2 = C.input_variable((4,))\n>>> in1_data = np.asarray([[1.,\
    \ 2., 3., 4.]], np.float32)\n>>> in2_data = np.asarray([[0., 5., -3., 2.]], np.float32)\n\
    >>> plus_operation = in1 + in2\n>>> minus_operation = in1 - in2\n>>> forward =\
    \ C.combine([plus_operation, minus_operation]).eval({in1: in1_data, in2: in2_data})\n\
    >>> len(forward)\n2\n>>> list(forward.values()) # doctest: +SKIP\n[array([[[ 1.,\
    \ -3.,  6.,  2.]]], dtype=float32),\n array([[[ 1.,  7.,  0.,  6.]]], dtype=float32)]"
  syntax:
    content: combine()
    parameters:
    - description: list of functions or their variables to combine
      id: operands
      type:
      - list
    - description: the name of the Combine Function in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.combine
- _type: function
  fullName: cntk.ops.constant
  langs:
  - python
  module: cntk.ops
  name: constant
  source:
    id: constant
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2648
  summary: "It creates a constant tensor initialized from a numpy array\n-[ Example\
    \ ]-\n>>> constant_data = C.constant([[1., 2.], [3., 4.], [5., 6.]])\n>>> constant_data.value\n\
    array([[ 1.,  2.],\n       [ 3.,  4.],\n       [ 5.,  6.]], dtype=float32)"
  syntax:
    content: constant()
    parameters:
    - description: a scalar initial value that would be replicated for every element
        in the tensor or NumPy array. If `None`, the tensor will be initialized uniformly
        random.
      id: value
      type:
      - scalar or NumPy array, optional
    - description: the shape of the input tensor. If not provided, it will be inferred
        from `value`.
      id: shape
      type:
      - tuple or int, optional
    - description: data type of the constant. If a NumPy array and `dtype`, are given,
        then data will be converted if needed. If none given, it will default to `np.float32`.
      id: dtype
      type:
      - optional
    - description: instance of DeviceDescriptor
      id: device
      type:
      - DeviceDescriptor
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Constant`'
  type: Method
  uid: cntk.ops.constant
- _type: function
  fullName: cntk.ops.convolution
  langs:
  - python
  module: cntk.ops
  name: convolution
  source:
    id: convolution
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 177
  summary: "Computes the convolution of `convolution_map` (typically a tensor of learnable\
    \ parameters) with `operand` (commonly an image or output of a previous convolution/pooling\
    \ operation). This operation is used in image and language processing applications.\
    \ It supports arbitrary dimensions, strides, sharing, and padding.\nThis function\
    \ operates on input tensors with dimensions . This can be understood as a rank-n\
    \ object, where each entry consists of a -dimensional vector. For example, an\
    \ RGB image would have dimensions , i.e. a -sized structure, where each entry\
    \ (pixel) consists of a 3-tuple.\n*convolution* convolves the input `operand`\
    \ with a  rank tensor of (typically learnable) filters called `convolution_map`\
    \ of shape  (typically ). The first dimension, , is the nunber of convolution\
    \ filters (i.e. the number of channels in the output). The second dimension, ,\
    \ must match the number of channels in the input. The last n dimensions are the\
    \ spatial extent of the filter. I.e. for each output position, a vector of dimension\
    \  is computed. Hence, the total number of filter parameters is\n-[ Example ]-\n\
    >>> img = np.reshape(np.arange(25.0, dtype = np.float32), (1, 5, 5))\n>>> x =\
    \ C.input_variable(img.shape)\n>>> filter = np.reshape(np.array([2, -1, -1, 2],\
    \ dtype = np.float32), (1, 2, 2))\n>>> kernel = C.constant(value = filter)\n>>>\
    \ np.round(C.convolution(kernel, x, auto_padding = [False]).eval({x: [img]}),5)\n\
    array([[[[[  6.,   8.,  10.,  12.],\n          [ 16.,  18.,  20.,  22.],\n   \
    \       [ 26.,  28.,  30.,  32.],\n          [ 36.,  38.,  40.,  42.]]]]], dtype=float32)"
  syntax:
    content: convolution()
    parameters:
    - description: convolution filter weights, stored as a tensor of dimensions ,
        where  must be the kernel dimensions (spatial extent of the filter).
      id: convolution_map
    - description: convolution input. A tensor with dimensions .
      id: operand
    - description: stride dimensions. If strides[i] > 1 then only pixel positions
        that are multiples of strides[i] are computed. For example, a stride of 2
        will lead to a halving of that dimension. The first stride dimension that
        lines up with the number of input channels can be set to any non-zero value.
      id: strides
      type:
      - tuple, optional
    - description: sharing flags for each input dimension
      id: sharing
      type:
      - bool
    - description: flags for each input dimension whether it should be padded automatically
        (that is, symmetrically) or not padded at all. Padding means that the convolution
        kernel is applied to all pixel positions, where all pixels outside the area
        are assumed zero ("padded with zeroes"). Without padding, the kernels are
        only shifted over positions where all inputs to the kernel still fall inside
        the area. In this case, the output dimension will be less than the input dimension.
        The last value that lines up with the number of input channels must be false.
      id: auto_padding
      type:
      - bool
    - description: precise lower padding for each input dimension.
      id: lower_pad
    - description: precise upper padding for each input dimension.
      id: upper_pad
    - description: maximum amount of auxiliary memory (in samples) that should be
        reserved to perform convolution operations. Some convolution engines (e.g.
        cuDNN and GEMM-based engines) can benefit from using workspace as it may improve
        performance. However, sometimes this may lead to higher memory utilization.
        Default is 0 which means the same as the input samples.
      id: max_temp_mem_size_in_samples
      type:
      - int
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.convolution
- _type: function
  fullName: cntk.ops.convolution_transpose
  langs:
  - python
  module: cntk.ops
  name: convolution_transpose
  source:
    id: convolution_transpose
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 241
  summary: "Computes the transposed convolution of `convolution_map` (typically a\
    \ tensor of learnable parameters) with `operand` (commonly an image or output\
    \ of a previous convolution/pooling operation). This is also known as `fractionally\
    \ strided convolutional layers`, or, `deconvolution`. This operation is used in\
    \ image and language processing applications. It supports arbitrary dimensions,\
    \ strides, sharing, and padding.\nThis function operates on input tensors with\
    \ dimensions . This can be understood as a rank-n object, where each entry consists\
    \ of a -dimensional vector. For example, an RGB image would have dimensions ,\
    \ i.e. a -sized structure, where each entry (pixel) consists of a 3-tuple.\n*convolution_transpose*\
    \ convolves the input `operand` with a  rank tensor of (typically learnable) filters\
    \ called `convolution_map` of shape  (typically ). The first dimension, , is the\
    \ nunber of convolution filters (i.e. the number of channels in the output). The\
    \ second dimension, , must match the number of channels in the input. The last\
    \ n dimensions are the spatial extent of the filter. I.e. for each output position,\
    \ a vector of dimension  is computed. Hence, the total number of filter parameters\
    \ is\n-[ Example ]-\n>>> img = np.reshape(np.arange(9.0, dtype = np.float32),\
    \ (1, 3, 3))\n>>> x = C.input_variable(img.shape)\n>>> filter = np.reshape(np.array([2,\
    \ -1, -1, 2], dtype = np.float32), (1, 2, 2))\n>>> kernel = C.constant(value =\
    \ filter)\n>>> np.round(C.convolution_transpose(kernel, x, auto_padding = [False]).eval({x:\
    \ [img]}),5)\narray([[[[[  0.,   2.,   3.,  -2.],\n          [  6.,   4.,   6.,\
    \  -1.],\n          [  9.,  10.,  12.,   2.],\n          [ -6.,   5.,   6.,  16.]]]]],\
    \ dtype=float32)"
  syntax:
    content: convolution_transpose()
    parameters:
    - description: convolution filter weights, stored as a tensor of dimensions ,
        where  must be the kernel dimensions (spatial extent of the filter).
      id: convolution_map
    - description: convolution input. A tensor with dimensions .
      id: operand
    - description: stride dimensions. If strides[i] > 1 then only pixel positions
        that are multiples of strides[i] are computed. For example, a stride of 2
        will lead to a halving of that dimension. The first stride dimension that
        lines up with the number of input channels can be set to any non-zero value.
      id: strides
      type:
      - tuple, optional
    - description: sharing flags for each input dimension
      id: sharing
      type:
      - bool
    - description: flags for each input dimension whether it should be padded automatically
        (that is, symmetrically) or not padded at all. Padding means that the convolution
        kernel is applied to all pixel positions, where all pixels outside the area
        are assumed zero ("padded with zeroes"). Without padding, the kernels are
        only shifted over positions where all inputs to the kernel still fall inside
        the area. In this case, the output dimension will be less than the input dimension.
        The last value that lines up with the number of input channels must be false.
      id: auto_padding
      type:
      - bool
    - description: precise lower padding for each input dimension.
      id: lower_pad
    - description: precise upper padding for each input dimension.
      id: upper_pad
    - description: user expected output shape after convolution transpose.
      id: output_shape
    - description: maximum amount of auxiliary memory (in samples) that should be
        reserved to perform convolution operations. Some convolution engines (e.g.
        cuDNN and GEMM-based engines) can benefit from using workspace as it may improve
        performance. However, sometimes this may lead to higher memory utilization.
        Default is 0 which means the same as the input samples.
      id: max_temp_mem_size_in_samples
      type:
      - int
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.convolution_transpose
- _type: function
  fullName: cntk.ops.cos
  langs:
  - python
  module: cntk.ops
  name: cos
  source:
    id: cos
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1369
  summary: "Computes the element-wise cosine of `x`:\nThe output tensor has the same\
    \ shape as `x`.\n-[ Example ]-\n>>> np.round(C.cos(np.arccos([[1,0.5],[-0.25,-0.75]])).eval(),5)\n\
    array([[ 1.  ,  0.5 ],\n       [-0.25, -0.75]], dtype=float32)"
  syntax:
    content: cos()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.cos
- _type: function
  fullName: cntk.ops.dropout
  langs:
  - python
  module: cntk.ops
  name: dropout
  source:
    id: dropout
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2462
  summary: "Each element of the input is independently set to 0 with probabily `dropout_rate`\
    \ or to 1 / (1 - `dropout_rate`) times its original value (with probability 1-`dropout_rate`).\
    \ Dropout is a good way to reduce overfitting.\nThis behavior only happens during\
    \ training. During inference dropout is a no-op. In the paper that introduced\
    \ dropout it was suggested to scale the weights during inference In CNTK's implementation,\
    \ because the values that are not set to 0 are multiplied with (1 / (1 - `dropout_rate`)),\
    \ this is not necessary.\n-[ Example ]-\n>>> data = [[10, 20],[30, 40],[50, 60]]\n\
    >>> C.dropout(data, 0.5).eval() # doctest: +SKIP\narray([[  0.,  40.],\n     \
    \  [  0.,  80.],\n       [  0.,   0.]], dtype=float32)\n>>> C.dropout(data, 0.75).eval()\
    \ # doctest: +SKIP\narray([[   0.,    0.],\n       [   0.,  160.],\n       [ \
    \  0.,  240.]], dtype=float32)"
  syntax:
    content: dropout()
    parameters:
    - description: input tensor
      id: x
    - description: probability that an element of `x` will be set to zero
      id: dropout_rate
      type:
      - float, [0,1)
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.dropout
- _type: function
  fullName: cntk.ops.element_divide
  langs:
  - python
  module: cntk.ops
  name: element_divide
  source:
    id: element_divide
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 819
  summary: 'The output of this operation is the element-wise division of the two input
    tensors. It supports broadcasting.

    -[ Example ]-

    >>> C.element_divide([1., 1., 1., 1.], [0.5, 0.25, 0.125, 0.]).eval()

    array([ 2.,  4.,  8.,  0.], dtype=float32)

    >>> C.element_divide([5., 10., 15., 30.], [2.]).eval()

    array([  2.5,   5. ,   7.5,  15. ], dtype=float32)'
  syntax:
    content: element_divide()
    parameters:
    - description: left side tensor
      id: left
    - description: right side tensor
      id: right
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.element_divide
- _type: function
  fullName: cntk.ops.element_max
  langs:
  - python
  module: cntk.ops
  name: element_max
  source:
    id: element_max
    path: cntk/ops/__init__.py
    remote:
      branch: master
      path: cntk/ops/__init__.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 779
  summary: The output of this operation is the element-wise max of the two or more
    input tensors. It supports broadcasting.
  syntax:
    content: element_max(arg1, arg2)
    parameters:
    - description: left side tensor
      id: arg1
    - description: right side tensor
      id: arg2
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.element_max
- _type: function
  fullName: cntk.ops.element_min
  langs:
  - python
  module: cntk.ops
  name: element_min
  source:
    id: element_min
    path: cntk/ops/__init__.py
    remote:
      branch: master
      path: cntk/ops/__init__.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 799
  summary: The output of this operation is the element-wise min of the two or more
    input tensors. It supports broadcasting.
  syntax:
    content: element_min(arg1, arg2)
    parameters:
    - description: left side tensor
      id: arg1
    - description: right side tensor
      id: arg2
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.element_min
- _type: function
  fullName: cntk.ops.element_select
  langs:
  - python
  module: cntk.ops
  name: element_select
  source:
    id: element_select
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1623
  summary: 'return either `value_if_true` or `value_if_false` based on the value of
    `flag`. If `flag` != 0 `value_if_true` is returned, otherwise `value_if_false`.
    Behaves analogously to numpy.where(...).

    -[ Example ]-

    >>> C.element_select([-10, -1, 0, 0.3, 100], [1, 10, 100, 1000, 10000], [ 2, 20,
    200, 2000, 20000]).eval()

    array([     1.,     10.,    200.,   1000.,  10000.], dtype=float32)'
  syntax:
    content: element_select()
    parameters:
    - description: condition tensor
      id: flag
    - description: true branch tensor
      id: value_if_true
    - description: false branch tensor
      id: value_if_false
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.element_select
- _type: function
  fullName: cntk.ops.element_times
  langs:
  - python
  module: cntk.ops
  name: element_times
  source:
    id: element_times
    path: cntk/ops/__init__.py
    remote:
      branch: master
      path: cntk/ops/__init__.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 746
  summary: 'The output of this operation is the element-wise product of the two or
    more input tensors. It supports broadcasting.

    -[ Example ]-

    >>> C.element_times([1., 1., 1., 1.], [0.5, 0.25, 0.125, 0.]).eval()

    array([ 0.5  ,  0.25 ,  0.125,  0.   ], dtype=float32)

    >>> C.element_times([5., 10., 15., 30.], [2.]).eval()

    array([ 10.,  20.,  30.,  60.], dtype=float32)

    >>> C.element_times([5., 10., 15., 30.], [2.], [1., 2., 1., 2.]).eval()

    array([  10.,   40.,   30.,  120.], dtype=float32)'
  syntax:
    content: element_times(arg1, arg2)
    parameters:
    - description: left side tensor
      id: arg1
    - description: right side tensor
      id: arg2
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.element_times
- _type: function
  fullName: cntk.ops.elu
  langs:
  - python
  module: cntk.ops
  name: elu
  source:
    id: elu
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1188
  summary: 'Exponential linear unit operation. Computes the element-wise exponential
    linear of `x`: `max(x, 0)` for `x >= 0` and `x`: `exp(x)-1` otherwise.

    The output tensor has the same shape as `x`.

    -[ Example ]-

    >>> C.elu([[-1, -0.5, 0, 1, 2]]).eval()

    array([[-0.632121, -0.393469,  0.      ,  1.      ,  2.      ]], dtype=float32)'
  syntax:
    content: elu()
    parameters:
    - description: any @cntk.ops.functions.Function that outputs a tensor.
      id: x
      type:
      - numpy.array or Function
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, default to ''
    return:
      description: An instance of @cntk.ops.functions.Function
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.ops.elu
- _type: function
  fullName: cntk.ops.equal
  langs:
  - python
  module: cntk.ops
  name: equal
  source:
    id: equal
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 503
  summary: 'Elementwise ''equal'' comparison of two tensors. Result is 1 if values
    are equal 0 otherwise.

    -[ Example ]-

    >>> C.equal([41., 42., 43.], [42., 42., 42.]).eval()

    array([ 0.,  1.,  0.], dtype=float32)

    >>> C.equal([-1,0,1], [1]).eval()

    array([ 0.,  0.,  1.], dtype=float32)'
  syntax:
    content: equal()
    parameters:
    - description: left side tensor
      id: left
    - description: right side tensor
      id: right
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.equal
- _type: function
  fullName: cntk.ops.exp
  langs:
  - python
  module: cntk.ops
  name: exp
  source:
    id: exp
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1465
  summary: 'Computes the element-wise exponential of `x`:


    -[ Example ]-

    >>> C.exp([0., 1.]).eval()

    array([ 1.      ,  2.718282], dtype=float32)'
  syntax:
    content: exp()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.exp
- _type: function
  fullName: cntk.ops.floor
  langs:
  - python
  module: cntk.ops
  name: floor
  source:
    id: floor
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1036
  summary: "The output of this operation is the element wise value rounded to the\
    \ largest integer less than or equal to the input.\n-[ Example ]-\n>>> C.floor([0.2,\
    \ 1.3, 4., 5.5, 0.0]).eval()\narray([ 0.,  1.,  4.,  5.,  0.], dtype=float32)\n\
    >>> C.floor([[0.6, 3.3], [1.9, 5.6]]).eval()\narray([[ 0.,  3.],\n       [ 1.,\
    \  5.]], dtype=float32)\n>>> C.floor([-5.5, -4.2, -3., -0.7, 0]).eval()\narray([-6.,\
    \ -5., -3., -1.,  0.], dtype=float32)\n>>> C.floor([[-0.6, -4.3], [1.9, -3.2]]).eval()\n\
    array([[-1., -5.],\n       [ 1., -4.]], dtype=float32)"
  syntax:
    content: floor()
    parameters:
    - description: input tensor
      id: arg
    - description: the name of the Function instance in the network (optional)
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.floor
- _type: function
  fullName: cntk.ops.forward_backward
  langs:
  - python
  module: cntk.ops
  name: forward_backward
  source:
    id: forward_backward
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 149
  summary: 'Criterion node for training methods that rely on forward-backward Viterbi-like
    passes, e.g. Connectionist Temporal Classification (CTC) training The node takes
    as the input the graph of labels, produced by the labels_to_graph operation that
    determines the exact forward/backward procedure. .. rubric:: Example

    graph = cntk.labels_to_graph(labels) networkOut = model(features) fb = C.forward_backward(graph,
    networkOut, 132)'
  syntax:
    content: forward_backward()
    parameters:
    - description: labels graph
      id: graph
    - description: network output
      id: features
    - description: id of the CTC blank label
      id: blankTokenId
    - description: label output delay constraint introduced during training that allows
        to have shorter delay during inference. This is using the original time information
        to enforce that CTC tokens only get aligned within a time margin. Setting
        this parameter smaller will result in shorted delay between label output during
        decoding, yet may hurt accuracy. delayConstraint=-1 means no constraint
      id: delayConstraint
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.forward_backward
- _type: function
  fullName: cntk.ops.future_value
  langs:
  - python
  module: cntk.ops
  name: future_value
  source:
    id: future_value
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1654
  summary: "This function returns the future value w.r.t. `x`. It is most often used\
    \ when creating RNNs. The resulting tensor has the same shape as the input but\
    \ is the next logical sample. The `time_step` parameter is the number of steps\
    \ to look into the future and is 1 by default. If there is no future value (i.e.\
    \ the current sample is the last one in the tensor) then the `initial_state` value\
    \ is returned.\nThe initial state can be a constant (scalar or tensor), a learnable\
    \ tensor or input data (which has a batch dimension, as needed for sequence-to-sequence\
    \ models).\n-[ Example ]-\n>>> x = C.input_variable(shape=(3,2))\n>>> # Create\
    \ one sequence with 4 tensors of shape (3, 2)\n>>> x0 = np.reshape(np.arange(24,dtype=np.float32),(1,4,3,2))\n\
    >>> y = C.future_value(x) # using initial state of 0 by default\n>>> y.eval({x:x0})\n\
    array([[[[  6.,   7.],\n         [  8.,   9.],\n         [ 10.,  11.]],\n<BLANKLINE>\n\
    \        [[ 12.,  13.],\n         [ 14.,  15.],\n         [ 16.,  17.]],\n<BLANKLINE>\n\
    \        [[ 18.,  19.],\n         [ 20.,  21.],\n         [ 22.,  23.]],\n<BLANKLINE>\n\
    \        [[  0.,   0.],\n         [  0.,   0.],\n         [  0.,   0.]]]], dtype=float32)"
  syntax:
    content: future_value()
    parameters:
    - description: the tensor (or its name) from which the future value is obtained.
      id: x
    - description: tensor or scalar representing the initial value to be used when
        the input tensor is shifted in time.
      id: initial_state
    - description: the number of time steps to look into the future (default 1)
      id: time_step
      type:
      - int
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.future_value
- _type: function
  fullName: cntk.ops.greater
  langs:
  - python
  module: cntk.ops
  name: greater
  source:
    id: greater
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 529
  summary: 'Elementwise ''greater'' comparison of two tensors. Result is 1 if left
    > right else 0.

    -[ Example ]-

    >>> C.greater([41., 42., 43.], [42., 42., 42.]).eval()

    array([ 0.,  0.,  1.], dtype=float32)

    >>> C.greater([-1,0,1], [0]).eval()

    array([ 0.,  0.,  1.], dtype=float32)'
  syntax:
    content: greater()
    parameters:
    - description: left side tensor
      id: left
    - description: right side tensor
      id: right
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.greater
- _type: function
  fullName: cntk.ops.greater_equal
  langs:
  - python
  module: cntk.ops
  name: greater_equal
  source:
    id: greater_equal
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 555
  summary: 'Elementwise ''greater equal'' comparison of two tensors. Result is 1 if
    left >= right else 0.

    -[ Example ]-

    >>> C.greater_equal([41., 42., 43.], [42., 42., 42.]).eval()

    array([ 0.,  1.,  1.], dtype=float32)

    >>> C.greater_equal([-1,0,1], [0]).eval()

    array([ 0.,  1.,  1.], dtype=float32)'
  syntax:
    content: greater_equal()
    parameters:
    - description: left side tensor
      id: left
    - description: right side tensor
      id: right
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.greater_equal
- _type: function
  fullName: cntk.ops.hardmax
  langs:
  - python
  module: cntk.ops
  name: hardmax
  source:
    id: hardmax
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1441
  summary: 'Creates a tensor with the same shape as the input tensor, with zeros everywhere
    and a 1.0 where the maximum value of the input tensor is located. If the maximum
    value is repeated, 1.0 is placed in the first location found.

    -[ Example ]-

    >>> C.hardmax([1., 1., 2., 3.]).eval()

    array([ 0.,  0.,  0.,  1.], dtype=float32)

    >>> C.hardmax([1., 3., 2., 3.]).eval()

    array([ 0.,  1.,  0.,  0.], dtype=float32)'
  syntax:
    content: hardmax()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.hardmax
- _type: function
  fullName: cntk.ops.input_variable
  langs:
  - python
  module: cntk.ops
  name: input_variable
  source:
    id: input_variable
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2509
  summary: 'It creates an input in the network: a place where data, such as features
    and labels, should be provided.'
  syntax:
    content: input_variable()
    parameters:
    - description: the shape of the input tensor
      id: shape
      type:
      - tuple or int
    - description: np.float32 (default) or np.float64
      id: dtype
      type:
      - type, optional
    - description: whether to back-propagates to it or not. False by default.
      id: needs_gradients
      type:
      - bool, optional
    - description: whether the variable is sparse (*False* by default)
      id: is_sparse
      type:
      - bool, optional
    - description: a list of dynamic axis (e.g., batch axis, time axis)
      id: dynamic_axes
      type:
      - list or tuple, default
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Variable`'
  type: Method
  uid: cntk.ops.input_variable
- _type: function
  fullName: cntk.ops.labels_to_graph
  langs:
  - python
  module: cntk.ops
  name: labels_to_graph
  source:
    id: labels_to_graph
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 129
  summary: 'Conversion node from labels to graph. Typically used as an input to ForwardBackward
    node. This node''s objective is to transform input labels into a graph representing
    exact forward-backward criterion. .. rubric:: Example

    num_classes = 2 labels = cntk.input_variable((num_classes)) graph = cntk.labels_to_graph(labels)'
  syntax:
    content: labels_to_graph()
    parameters:
    - description: input training labels
      id: labels
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.labels_to_graph
- _type: function
  fullName: cntk.ops.leaky_relu
  langs:
  - python
  module: cntk.ops
  name: leaky_relu
  source:
    id: leaky_relu
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1213
  summary: 'Leaky Rectified linear operation. Computes the element-wise leaky rectified
    linear of `x`: `max(x, 0)` for `x >= 0` and `x`: `0.01*x` otherwise.

    The output tensor has the same shape as `x`.

    -[ Example ]-

    >>> C.leaky_relu([[-1, -0.5, 0, 1, 2]]).eval()

    array([[-0.01 , -0.005,  0.   ,  1.   ,  2.   ]], dtype=float32)'
  syntax:
    content: leaky_relu()
    parameters:
    - description: any @cntk.ops.functions.Function that outputs a tensor.
      id: x
      type:
      - numpy.array or Function
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, default to ''
    return:
      description: An instance of @cntk.ops.functions.Function
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.ops.leaky_relu
- _type: function
  fullName: cntk.ops.less
  langs:
  - python
  module: cntk.ops
  name: less
  source:
    id: less
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 477
  summary: 'Elementwise ''less'' comparison of two tensors. Result is 1 if left <
    right else 0.

    -[ Example ]-

    >>> C.less([41., 42., 43.], [42., 42., 42.]).eval()

    array([ 1.,  0.,  0.], dtype=float32)

    >>> C.less([-1,0,1], [0]).eval()

    array([ 1.,  0.,  0.], dtype=float32)'
  syntax:
    content: less()
    parameters:
    - description: left side tensor
      id: left
    - description: right side tensor
      id: right
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.less
- _type: function
  fullName: cntk.ops.less_equal
  langs:
  - python
  module: cntk.ops
  name: less_equal
  source:
    id: less_equal
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 607
  summary: 'Elementwise ''less equal'' comparison of two tensors. Result is 1 if left
    <= right else 0.

    -[ Example ]-

    >>> C.less_equal([41., 42., 43.], [42., 42., 42.]).eval()

    array([ 1.,  1.,  0.], dtype=float32)

    >>> C.less_equal([-1,0,1], [0]).eval()

    array([ 1.,  1.,  0.], dtype=float32)'
  syntax:
    content: less_equal()
    parameters:
    - description: left side tensor
      id: left
    - description: right side tensor
      id: right
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.less_equal
- _type: function
  fullName: cntk.ops.log
  langs:
  - python
  module: cntk.ops
  name: log
  source:
    id: log
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1487
  summary: 'Computes the element-wise the natural logarithm of `x`:

    -[ Example ]-

    >>> C.log([1., 2.]).eval()

    array([ 0.      ,  0.693147], dtype=float32)

    Note: CNTK returns -85.1 for log(x) if `x` is negative or zero. The reason is
    that it uses 1e-37 (whose natural logarithm is -85.1) as the smallest float number
    for *log*, because this is the only guaranteed precision across platforms. This
    will be changed to return *NaN* and *-inf*.'
  syntax:
    content: log()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.log
- _type: function
  fullName: cntk.ops.log_add_exp
  langs:
  - python
  module: cntk.ops
  name: log_add_exp
  source:
    id: log_add_exp
    path: cntk/ops/__init__.py
    remote:
      branch: master
      path: cntk/ops/__init__.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 846
  summary: 'Calculates the log of the sum of the exponentials of the two or more input
    tensors. It supports broadcasting.

    -[ Example ]-

    >>> a = np.arange(3,dtype=np.float32)

    >>> np.exp(C.log_add_exp(np.log(1+a), np.log(1+a*a)).eval())

    array([ 2.,  4.,  8.], dtype=float32)

    >>> np.exp(C.log_add_exp(np.log(1+a), [0.]).eval())

    array([ 2.,  3.,  4.], dtype=float32)'
  syntax:
    content: log_add_exp(arg1, arg2)
    parameters:
    - description: left side tensor
      id: arg1
    - description: right side tensor
      id: arg2
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.log_add_exp
- _type: function
  fullName: cntk.ops.minus
  langs:
  - python
  module: cntk.ops
  name: minus
  source:
    id: minus
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 718
  summary: "The output of this operation is left minus right tensor. It supports broadcasting.\n\
    -[ Example ]-\n>>> C.minus([1, 2, 3], [4, 5, 6]).eval()\narray([-3., -3., -3.],\
    \ dtype=float32)\n>>> C.minus([[1,2],[3,4]], 1).eval()\narray([[ 0.,  1.],\n \
    \      [ 2.,  3.]], dtype=float32)"
  syntax:
    content: minus()
    parameters:
    - description: left side tensor
      id: left
    - description: right side tensor
      id: right
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.minus
- _type: function
  fullName: cntk.ops.negate
  langs:
  - python
  module: cntk.ops
  name: negate
  source:
    id: negate
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1581
  summary: 'Computes the element-wise negation of `x`:


    -[ Example ]-

    >>> C.negate([-1, 1, -2, 3]).eval()

    array([ 1., -1.,  2., -3.], dtype=float32)'
  syntax:
    content: negate()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.negate
- _type: function
  fullName: cntk.ops.not_equal
  langs:
  - python
  module: cntk.ops
  name: not_equal
  source:
    id: not_equal
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 581
  summary: 'Elementwise ''not equal'' comparison of two tensors. Result is 1 if left
    != right else 0.

    -[ Example ]-

    >>> C.not_equal([41., 42., 43.], [42., 42., 42.]).eval()

    array([ 1.,  0.,  1.], dtype=float32)

    >>> C.not_equal([-1,0,1], [0]).eval()

    array([ 1.,  0.,  1.], dtype=float32)'
  syntax:
    content: not_equal()
    parameters:
    - description: left side tensor
      id: left
    - description: right side tensor
      id: right
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.not_equal
- _type: function
  fullName: cntk.ops.optimized_rnnstack
  langs:
  - python
  module: cntk.ops
  name: optimized_rnnstack
  source:
    id: optimized_rnnstack
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1813
  summary: "An RNN implementation that uses the primitives in cuDNN. If cuDNN is not\
    \ available it fails.\n-[ Example ]-\n>>> from _cntk_py import InferredDimension,\
    \ constant_initializer\n>>> W = C.parameter((InferredDimension,4), constant_initializer(0.1))\n\
    >>> x = C.input_variable(shape=(4,))\n>>> s = np.reshape(np.arange(20.0, dtype=np.float32),\
    \ (5,4))\n>>> t = np.reshape(np.arange(12.0, dtype=np.float32), (3,4))\n>>> f\
    \ = C.optimized_rnnstack(x, W, 8, 2) # doctest: +SKIP\n>>> r = f.eval({x:[s,t]})\
    \                # doctest: +SKIP\n>>> len(r)                               #\
    \ doctest: +SKIP\n2\n>>> print(*r[0].shape)                   # doctest: +SKIP\n\
    5 8\n>>> print(*r[1].shape)                   # doctest: +SKIP\n3 8\n>>> r[0][:3,:]-r[1]\
    \                      # doctest: +SKIP\narray([[ 0.,  0.,  0.,  0.,  0.,  0.,\
    \  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0., \
    \ 0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
  syntax:
    content: optimized_rnnstack()
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.optimized_rnnstack
- _type: function
  fullName: cntk.ops.output_variable
  langs:
  - python
  module: cntk.ops
  name: output_variable
  source:
    id: output_variable
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2542
  summary: It creates an output variable that is used to define a user defined function.
  syntax:
    content: output_variable()
    parameters:
    - description: the shape of the input tensor
      id: shape
      type:
      - tuple or int
    - description: data type
      id: dtype
      type:
      - np.float32 or np.float64
    - description: a list of dynamic axis (e.g., batch axis, time axis)
      id: dynamic_axes
      type:
      - list or tuple
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Variable` that is of output type'
  type: Method
  uid: cntk.ops.output_variable
- _type: function
  fullName: cntk.ops.param_relu
  langs:
  - python
  module: cntk.ops
  name: param_relu
  source:
    id: param_relu
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1237
  summary: 'Parametric rectified linear operation. Computes the element-wise parameteric
    rectified linear of `x`: `max(x, 0)` for `x >= 0` and `x`: `alpha*x` otherwise.

    The output tensor has the same shape as `x`.

    -[ Example ]-

    >>> alpha = C.constant(value=[[0.5, 0.5, 0.5, 0.5, 0.5]])

    >>> C.param_relu(alpha, [[-1, -0.5, 0, 1, 2]]).eval()

    array([[-0.5 , -0.25,  0.  ,  1.  ,  2.  ]], dtype=float32)'
  syntax:
    content: param_relu()
    parameters:
    - description: same shape as x
      id: alpha
      type:
      - Parameter
    - description: any @cntk.ops.functions.Function that outputs a tensor.
      id: x
      type:
      - numpy.array or Function
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, default to ''
    return:
      description: An instance of @cntk.ops.functions.Function
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.ops.param_relu
- _type: function
  fullName: cntk.ops.parameter
  langs:
  - python
  module: cntk.ops
  name: parameter
  source:
    id: parameter
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2598
  summary: "It creates a parameter tensor.\n-[ Example ]-\n>>> init_parameter = C.parameter(shape=(3,4),\
    \ init=2)\n>>> np.asarray(init_parameter) # doctest: +SKIP\narray([[ 2.,  2.,\
    \  2.,  2.],\n       [ 2.,  2.,  2.,  2.],\n       [ 2.,  2.,  2.,  2.]], dtype=float32)"
  syntax:
    content: parameter()
    parameters:
    - description: the shape of the input tensor. If not provided, it will be inferred
        from `value`.
      id: shape
      type:
      - tuple or int, optional
    - description: if init is a scalar it will be replicated for every element in
        the tensor or NumPy array. If it is the output of an initializer form [cntk.initializer](.md#module-cntk.initializer.md)
        it will be used to initialize the tensor at the first forward pass. If *None*,
        the tensor will be initialized with 0.
      id: init
      type:
      - scalar or NumPy array or initializer
    - description: data type of the constant. If a NumPy array and `dtype`, are given,
        then data will be converted if needed. If none given, it will default to `np.float32`.
      id: dtype
      type:
      - optional
    - description: instance of DeviceDescriptor
      id: device
      type:
      - DeviceDescriptor
    - description: the name of the Parameter instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Parameter`'
  type: Method
  uid: cntk.ops.parameter
- _type: function
  fullName: cntk.ops.past_value
  langs:
  - python
  module: cntk.ops
  name: past_value
  source:
    id: past_value
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1709
  summary: "This function returns the past value w.r.t. `x`. It is most often used\
    \ when creating RNNs. The resulting tensor has the same shape as the input but\
    \ is the previous logical sample. The `time_step` parameter is the number of steps\
    \ to look into the past and is 1 by default. If there is no past value (i.e. the\
    \ current sample is the first one in the tensor)  then the `initial_state` value\
    \ is returned.\nThe initial state can be a constant (scalar or tensor), a learnable\
    \ tensor or input data (which has a batch dimension, as needed for sequence-to-sequence\
    \ models).\n-[ Example ]-\n>>> # create example input: one sequence with 4 tensors\
    \ of shape (3, 2)\n>>> from cntk.layers import Input\n>>> from cntk.layers.typing\
    \ import Tensor, Sequence\n>>> x = Input(**Sequence[Tensor[3,2]])\n>>> x0 = np.reshape(np.arange(24,dtype=np.float32),(1,4,3,2))\n\
    >>> x0\narray([[[[  0.,   1.],\n         [  2.,   3.],\n         [  4.,   5.]],\n\
    <BLANKLINE>\n        [[  6.,   7.],\n         [  8.,   9.],\n         [ 10., \
    \ 11.]],\n<BLANKLINE>\n        [[ 12.,  13.],\n         [ 14.,  15.],\n      \
    \   [ 16.,  17.]],\n<BLANKLINE>\n        [[ 18.,  19.],\n         [ 20.,  21.],\n\
    \         [ 22.,  23.]]]], dtype=float32)\n>>> # this demonstrates how past_value\
    \ shifts the sequence by one, padding with initial_state\n>>> y = C.past_value(x)\
    \ # initial_state is 0 by default\n>>> y.eval({x:x0})\narray([[[[  0.,   0.],\n\
    \         [  0.,   0.],\n         [  0.,   0.]],\n<BLANKLINE>\n        [[  0.,\
    \   1.],\n         [  2.,   3.],\n         [  4.,   5.]],\n<BLANKLINE>\n     \
    \   [[  6.,   7.],\n         [  8.,   9.],\n         [ 10.,  11.]],\n<BLANKLINE>\n\
    \        [[ 12.,  13.],\n         [ 14.,  15.],\n         [ 16.,  17.]]]], dtype=float32)\n\
    >>> # here, we pass a the initial_state as input data (e.g. sequence-to-sequence)\n\
    >>> s = Input(**Tensor[3,2])  # not a Sequence[], e.g. a final encoder hidden\
    \ state\n>>> s0 = np.reshape(np.arange(6,dtype=np.float32)/2,(1,1,3,2))\n>>> s0\n\
    array([[[[ 0. ,  0.5],\n         [ 1. ,  1.5],\n         [ 2. ,  2.5]]]], dtype=float32)\n\
    >>> y = C.past_value(x, initial_state=s)\n>>> y.eval({x:x0, s:s0}) # same as the\
    \ previous example except for the first time step\narray([[[[  0. ,   0.5],\n\
    \         [  1. ,   1.5],\n         [  2. ,   2.5]],\n<BLANKLINE>\n        [[\
    \  0. ,   1. ],\n         [  2. ,   3. ],\n         [  4. ,   5. ]],\n<BLANKLINE>\n\
    \        [[  6. ,   7. ],\n         [  8. ,   9. ],\n         [ 10. ,  11. ]],\n\
    <BLANKLINE>\n        [[ 12. ,  13. ],\n         [ 14. ,  15. ],\n         [ 16.\
    \ ,  17. ]]]], dtype=float32)"
  syntax:
    content: past_value()
    parameters:
    - description: the tensor (or its name) from which the past value is obtained
      id: x
    - description: tensor or scalar representing the initial value to be used when
        the input tensor is shifted in time.
      id: initial_state
    - description: the number of time steps to look into the past (default 1)
      id: time_step
      type:
      - int
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.past_value
- _type: function
  fullName: cntk.ops.per_dim_mean_variance_normalize
  langs:
  - python
  module: cntk.ops
  name: per_dim_mean_variance_normalize
  source:
    id: per_dim_mean_variance_normalize
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2696
  summary: Computes per dimension mean-variance normalization of the specified input
    operand.
  syntax:
    content: per_dim_mean_variance_normalize()
    parameters:
    - description: the variable to be normalized
      id: operand
    - description: per dimension mean to use for the normalization
      id: mean
      type:
      - NumPy array
    - description: per dimension standard deviation to use for the normalization
      id: inv_stddev
      type:
      - NumPy array
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.per_dim_mean_variance_normalize
- _type: function
  fullName: cntk.ops.placeholder_variable
  langs:
  - python
  module: cntk.ops
  name: placeholder_variable
  source:
    id: placeholder_variable
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2570
  summary: It creates a placeholder variable that has to be later bound to an actual
    variable. A common use of this is to serve as a placeholder for a later output
    variable in a recurrent network, which is replaced with the actual output variable
    by calling replace_placeholder(s).
  syntax:
    content: placeholder_variable()
    parameters:
    - description: the shape of the variable tensor
      id: shape
      type:
      - tuple or int
    - description: the list of dynamic axes that the actual variable uses
      id: dynamic_axes
      type:
      - list
    return:
      description: '`Variable`'
  type: Method
  uid: cntk.ops.placeholder_variable
- _type: function
  fullName: cntk.ops.plus
  langs:
  - python
  module: cntk.ops
  name: plus
  source:
    id: plus
    path: cntk/ops/__init__.py
    remote:
      branch: master
      path: cntk/ops/__init__.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 684
  summary: 'The output of this operation is the sum of the two or more input tensors.
    It supports broadcasting.

    -[ Example ]-

    >>> C.plus([1, 2, 3], [4, 5, 6]).eval()

    array([ 5.,  7.,  9.], dtype=float32)

    >>> C.plus([-5, -4, -3, -2, -1], [10]).eval()

    array([ 5.,  6.,  7.,  8.,  9.], dtype=float32)

    >>> C.plus([-5, -4, -3, -2, -1], [10], [3, 2, 3, 2, 3], [-13], [+42], ''multi_arg_example'').eval()

    array([ 37.,  37.,  39.,  39.,  41.], dtype=float32)

    >>> C.plus([-5, -4, -3, -2, -1], [10], [3, 2, 3, 2, 3]).eval()

    array([  8.,   8.,  10.,  10.,  12.], dtype=float32)'
  syntax:
    content: plus(arg1, arg2)
    parameters:
    - description: left side tensor
      id: arg1
    - description: right side tensor
      id: arg2
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.plus
- _type: function
  fullName: cntk.ops.pooling
  langs:
  - python
  module: cntk.ops
  name: pooling
  source:
    id: pooling
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 340
  summary: "The pooling operations compute a new tensor by selecting the maximum or\
    \ average value in the pooling input. In the case of average pooling with padding,\
    \ the average is only over the valid region.\nN-dimensional pooling allows to\
    \ create max or average pooling of any dimensions, stride or padding.\n-[ Example\
    \ ]-\n>>> img = np.reshape(np.arange(16, dtype = np.float32), [1, 4, 4])\n>>>\
    \ x = C.input_variable(img.shape)\n>>> C.pooling(x, C.AVG_POOLING, (2,2), (2,2)).eval({x\
    \ : [img]})\narray([[[[[  2.5,   4.5],\n          [ 10.5,  12.5]]]]], dtype=float32)\n\
    >>> C.pooling(x, C.MAX_POOLING, (2,2), (2,2)).eval({x : [img]})\narray([[[[[ \
    \ 5.,   7.],\n          [ 13.,  15.]]]]], dtype=float32)"
  syntax:
    content: pooling()
    parameters:
    - description: pooling input
      id: operand
    - description: one of `MAX_POOLING` or `AVG_POOLING`
      id: pooling_type
    - description: dimensions of the pooling window
      id: pooling_window_shape
    - description: strides.
      id: strides
      type:
      - default 1
    - description: automatic padding flags for each input dimension.
      id: auto_padding
      type:
      - default [False,]
    - description: precise lower padding for each input dimension
      id: lower_pad
      type:
      - default (0,)
    - description: precise upper padding for each input dimension
      id: upper_pad
      type:
      - default (0,)
    - description: ceiling while computing output size
      id: ceil_out_dim
      type:
      - default false
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.pooling
- _type: function
  fullName: cntk.ops.random_sample
  langs:
  - python
  module: cntk.ops
  name: random_sample
  source:
    id: random_sample
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2371
  summary: 'Estimates inclusion frequencies for random sampling with or without replacement.

    The output value is a set of num_samples random samples represented by a (sparse)
    matrix of shape [num_samples x len(weights)], where len(weights) is the number
    of classes (categories) to choose from. The output has no dynamic axis. The samples
    are drawn according to the weight vector p(i) = weights[i] / sum(weights) We get
    one set of samples per minibatch. Intended use cases are e.g. sampled softmax,
    noise contrastive estimation etc.'
  syntax:
    content: random_sample()
    parameters:
    - description: input vector of sampling weights which should be non-negative numbers.
      id: weights
    - description: number of expected samples
      id: num_samples
      type:
      - int
    - description: If sampling is done with replacement (*True*) or without (*False*).
      id: allow_duplicates
      type:
      - bool
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.random_sample
- _type: function
  fullName: cntk.ops.random_sample_inclusion_frequency
  langs:
  - python
  module: cntk.ops
  name: random_sample_inclusion_frequency
  source:
    id: random_sample_inclusion_frequency
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2404
  summary: 'For weighted sampling with the specifed sample size (*num_samples*) this
    operation computes the expected number of occurences of each class in the the
    sampled set. In case of sampling without replacement the result is only an estimate
    which might be quite rough in the case of small sample sizes. Intended uses are
    e.g. sampled softmax, noise contrastive estimation etc. This operation will be
    typically used together with @cntk.ops.random_sample.

    -[ Example ]-

    >>> import numpy as np

    >>> from cntk import *

    >>> # weight vector with 100 ''1000''-values followed

    >>> # by 100 ''1'' values

    >>> w1 = np.full((100),1000, dtype = np.float)

    >>> w2 = np.full((100),1, dtype = np.float)

    >>> w = np.concatenate((w1, w2))

    >>> f = random_sample_inclusion_frequency(w, 150, True).eval()

    >>> f[0]

    1.4985015

    >>> f[1]

    1.4985015

    >>> f[110]

    0.0014985015

    >>> # when switching to sampling without duplicates samples are

    >>> # forced to pick the low weight classes too

    >>> f = random_sample_inclusion_frequency(w, 150, False).eval()

    >>> f[0]

    1.0'
  syntax:
    content: random_sample_inclusion_frequency()
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.random_sample_inclusion_frequency
- _type: function
  fullName: cntk.ops.reciprocal
  langs:
  - python
  module: cntk.ops
  name: reciprocal
  source:
    id: reciprocal
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1603
  summary: 'Computes the element-wise reciprocal of `x`:

    -[ Example ]-

    >>> C.reciprocal([-1/3, 1/5, -2, 3]).eval()

    array([-3.      ,  5.      , -0.5     ,  0.333333], dtype=float32)'
  syntax:
    content: reciprocal()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.reciprocal
- _type: function
  fullName: cntk.ops.reduce_log_sum_exp
  langs:
  - python
  module: cntk.ops
  name: reduce_log_sum_exp
  seealso: 'See also: `reduce_sum()` for more details and examples.'
  source:
    id: reduce_log_sum_exp
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2133
  summary: 'Computes the log of the sum of the exponentiations of the input tensor''s
    elements across the specified axis.

    -[ Example ]-

    >>> x = C.input_variable(shape=(3,2))

    >>> val = np.reshape(np.arange(6.0, dtype=np.float32), (3,2))

    >>> lse = C.reduce_log_sum_exp(x)

    >>> lse.eval({x:[val]})

    array([[ 5.456193]], dtype=float32)

    >>> np.log(np.sum(np.exp(val)))

    5.4561934'
  syntax:
    content: reduce_log_sum_exp()
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.reduce_log_sum_exp
- _type: function
  fullName: cntk.ops.reduce_max
  langs:
  - python
  module: cntk.ops
  name: reduce_max
  seealso: 'See also: `reduce_sum()` for more details and examples.'
  source:
    id: reduce_max
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2200
  summary: "Computes the max of the input tensor's elements across the specified axis.\n\
    -[ Example ]-\n>>> # create 3x2 matrix in a sequence of length 1 in a batch of\
    \ one sample\n>>> data = [[10, 20],[30, 40],[50, 60]]\n>>> C.reduce_max(data,\
    \ 0).eval()\narray([[ 50.,  60.]], dtype=float32)\n>>> C.reduce_max(data, 1).eval()\n\
    array([[ 20.],\n       [ 40.],\n       [ 60.]], dtype=float32)"
  syntax:
    content: reduce_max()
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.reduce_max
- _type: function
  fullName: cntk.ops.reduce_mean
  langs:
  - python
  module: cntk.ops
  name: reduce_mean
  seealso: 'See also: `reduce_sum()` for more details and examples.'
  source:
    id: reduce_mean
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2166
  summary: "Computes the mean of the input tensor's elements across the specified\
    \ axis.\n-[ Example ]-\n>>> # create 3x2 matrix in a sequence of length 1 in a\
    \ batch of one sample\n>>> data = [[5, 20],[30, 40],[55, 60]]\n>>> C.reduce_mean(data,\
    \ 0).eval()\narray([[ 30.,  40.]], dtype=float32)\n>>> C.reduce_mean(data, 1).eval()\n\
    array([[ 12.5],\n       [ 35. ],\n       [ 57.5]], dtype=float32)"
  syntax:
    content: reduce_mean()
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.reduce_mean
- _type: function
  fullName: cntk.ops.reduce_min
  langs:
  - python
  module: cntk.ops
  name: reduce_min
  seealso: 'See also: `reduce_sum()` for more details and examples.'
  source:
    id: reduce_min
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2234
  summary: "Computes the min of the input tensor's elements across the specified axis.\n\
    -[ Example ]-\n>>> # create 3x2 matrix in a sequence of length 1 in a batch of\
    \ one sample\n>>> data = [[10, 20],[30, 40],[50, 60]]\n>>> C.reduce_min(data,\
    \ 0).eval()\narray([[ 10.,  20.]], dtype=float32)\n>>> C.reduce_min(data, 1).eval()\n\
    array([[ 10.],\n       [ 30.],\n       [ 50.]], dtype=float32)"
  syntax:
    content: reduce_min()
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.reduce_min
- _type: function
  fullName: cntk.ops.reduce_prod
  langs:
  - python
  module: cntk.ops
  name: reduce_prod
  seealso: 'See also: `reduce_sum()` for more details and examples.'
  source:
    id: reduce_prod
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2268
  summary: "Computes the min of the input tensor's elements across the specified axis.\n\
    -[ Example ]-\n>>> # create 3x2 matrix in a sequence of length 1 in a batch of\
    \ one sample\n>>> data = [[1, 2],[3, 4],[5, 6]]\n>>> C.reduce_prod(data, 0).eval()\n\
    array([[ 15.,  48.]], dtype=float32)\n>>> C.reduce_prod(data, 1).eval()\narray([[\
    \  2.],\n       [ 12.],\n       [ 30.]], dtype=float32)"
  syntax:
    content: reduce_prod()
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.reduce_prod
- _type: function
  fullName: cntk.ops.reduce_sum
  langs:
  - python
  module: cntk.ops
  name: reduce_sum
  source:
    id: reduce_sum
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2068
  summary: "Computes the sum of the input tensor's elements across one axis. If the\
    \ axis parameter is not specified then the sum will be computed over all static\
    \ axes, which is equivalent with specifying `axis=Axis.all_static_axes()`. If\
    \ `axis=Axis.all_axes()`, the output is a scalar which is the sum of all the elements\
    \ in the minibatch.\n-[ Example ]-\n>>> x = C.input_variable((2,2))\n>>> # create\
    \ a batch of 2 sequences each containing 2 2x2 matrices\n>>> x0 = np.arange(16,dtype=np.float32).reshape(2,2,2,2)\n\
    >>> # reduce over all static axes\n>>> C.reduce_mean(x).eval({x:x0})\narray([[\
    \  1.5,   5.5],\n       [  9.5,  13.5]], dtype=float32)\n>>> # reduce over specified\
    \ axes\n>>> C.reduce_mean(x,axis=0).eval({x:x0})\narray([[[[  1.,   2.]],\n<BLANKLINE>\n\
    \        [[  5.,   6.]]],\n<BLANKLINE>\n<BLANKLINE>\n       [[[  9.,  10.]],\n\
    <BLANKLINE>\n        [[ 13.,  14.]]]], dtype=float32)\n>>> C.reduce_mean(x,axis=1).eval({x:x0})\n\
    array([[[[  0.5],\n         [  2.5]],\n<BLANKLINE>\n        [[  4.5],\n      \
    \   [  6.5]]],\n<BLANKLINE>\n<BLANKLINE>\n       [[[  8.5],\n         [ 10.5]],\n\
    <BLANKLINE>\n        [[ 12.5],\n         [ 14.5]]]], dtype=float32)\n>>> # reduce\
    \ over all axes\n>>> np.round(C.reduce_mean(x, axis=C.Axis.all_axes()).eval({x:x0}),5)\n\
    7.5\n>>> # reduce over all axes when the batch has sequences of different length\n\
    >>> x1 = np.arange(4,dtype=np.float32).reshape(1,2,2)\n>>> x2 = np.arange(12,dtype=np.float32).reshape(3,2,2)\n\
    >>> np.round(C.reduce_mean(x, axis=C.Axis.all_axes()).eval({x:[x1,x2]}),5)\n4.5\n\
    >>> (np.sum(x1)+np.sum(x2))/(x1.size+x2.size)\n4.5"
  syntax:
    content: reduce_sum()
    parameters:
    - description: input tensor
      id: x
    - description: axis along which the reduction will be performed
      id: axis
      type:
      - int or Axis
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.reduce_sum
- _type: function
  fullName: cntk.ops.relu
  langs:
  - python
  module: cntk.ops
  name: relu
  source:
    id: relu
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1165
  summary: 'Rectified linear operation. Computes the element-wise rectified linear
    of `x`: `max(x, 0)`

    The output tensor has the same shape as `x`.

    -[ Example ]-

    >>> C.relu([[-1, -0.5, 0, 1, 2]]).eval()

    array([[ 0.,  0.,  0.,  1.,  2.]], dtype=float32)'
  syntax:
    content: relu()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.relu
- _type: function
  fullName: cntk.ops.reshape
  langs:
  - python
  module: cntk.ops
  name: reshape
  source:
    id: reshape
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1870
  summary: "Reinterpret input samples as having different tensor dimensions One dimension\
    \ may be specified as 0 and will be inferred\nThe output tensor has the shape\
    \ specified by 'shape'.\n-[ Example ]-\n>>> i1 = C.input_variable(shape=(3,2))\n\
    >>> C.reshape(i1, (2,3)).eval({i1:np.asarray([[[[0., 1.],[2., 3.],[4., 5.]]]],\
    \ dtype=np.float32)})\narray([[[[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.]]]],\
    \ dtype=float32)"
  syntax:
    content: reshape()
    parameters:
    - description: tensor to be reshaped
      id: x
    - description: a tuple defining the resulting shape
      id: shape
      type:
      - tuple
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.reshape
- _type: function
  fullName: cntk.ops.roipooling
  langs:
  - python
  module: cntk.ops
  name: roipooling
  source:
    id: roipooling
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 311
  summary: 'The ROI (Region of Interest) pooling operation pools over sub-regions
    of an input volume and produces a fixed sized output volume regardless of the
    ROI size. It is used for example for object detection.

    Each input image has a fixed number of regions of interest, which are specified
    as bounding boxes (x, y, w, h) that are relative to the image size [W x H]. This
    operation can be used as a replacement for the final pooling layer of an image
    classification network (as presented in Fast R-CNN and others).'
  syntax:
    content: roipooling()
    parameters:
    - description: a convolutional feature map as the input volume ([W x H x C x N]).
      id: conv_feature_map
    - description: the coordinates of the ROIs per image ([4 x roisPerImage x N]),
        each ROI is (x, y, w, h) relative to original image size.
      id: rois
    - description: dimensions (width x height) of the ROI pooling output shape
      id: roi_output_shape
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.roipooling
- _type: function
  fullName: cntk.ops.round
  langs:
  - python
  module: cntk.ops
  name: round
  source:
    id: round
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1093
  summary: "The output of this operation is the element wise value rounded to the\
    \ nearest integer. In case of tie, where element can have exact fractional part\
    \ of 0.5 this operation follows \"round half-up\" tie breaking strategy. This\
    \ is different from the round operation of numpy which follows round half to even.\n\
    -[ Example ]-\n>>> C.round([0.2, 1.3, 4., 5.5, 0.0]).eval()\narray([ 0.,  1.,\
    \  4.,  6.,  0.], dtype=float32)\n>>> C.round([[0.6, 3.3], [1.9, 5.6]]).eval()\n\
    array([[ 1.,  3.],\n       [ 2.,  6.]], dtype=float32)\n>>> C.round([-5.5, -4.2,\
    \ -3., -0.7, 0]).eval()\narray([-5., -4., -3., -1.,  0.], dtype=float32)\n>>>\
    \ C.round([[-0.6, -4.3], [1.9, -3.2]]).eval()\narray([[-1., -4.],\n       [ 2.,\
    \ -3.]], dtype=float32)"
  syntax:
    content: round()
    parameters:
    - description: input tensor
      id: arg
    - description: the name of the Function instance in the network (optional)
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.round
- _type: function
  fullName: cntk.ops.sigmoid
  langs:
  - python
  module: cntk.ops
  name: sigmoid
  source:
    id: sigmoid
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1301
  summary: 'Computes the element-wise sigmoid of `x`:


    The output tensor has the same shape as `x`.

    -[ Example ]-

    >>> C.sigmoid([-2, -1., 0., 1., 2.]).eval()

    array([ 0.119203,  0.268941,  0.5     ,  0.731059,  0.880797], dtype=float32)'
  syntax:
    content: sigmoid()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.sigmoid
- _type: function
  fullName: cntk.ops.sin
  langs:
  - python
  module: cntk.ops
  name: sin
  source:
    id: sin
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1347
  summary: "Computes the element-wise sine of `x`:\nThe output tensor has the same\
    \ shape as `x`.\n-[ Example ]-\n>>> np.round(C.sin(np.arcsin([[1,0.5],[-0.25,-0.75]])).eval(),5)\n\
    array([[ 1.  ,  0.5 ],\n       [-0.25, -0.75]], dtype=float32)"
  syntax:
    content: sin()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.sin
- _type: function
  fullName: cntk.ops.slice
  langs:
  - python
  module: cntk.ops
  name: slice
  seealso: 'See also: Indexing in NumPy: [http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html](http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)'
  source:
    id: slice
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1949
  summary: "Slice the input along an axis.\n-[ Example ]-\n>>> # slice using input\
    \ variable\n>>> # create 2x3 matrix\n>>> x1 = C.input_variable((2,3))\n>>> # slice\
    \ index 1 (second) at first axis\n>>> C.slice(x1, 0, 1, 2).eval({x1: np.asarray([[[[1,2,-3],\n\
    ...                                              [4, 5, 6]]]],dtype=np.float32)})\n\
    array([[[[ 4.,  5.,  6.]]]], dtype=float32)\n<BLANKLINE>\n>>> # slice index 0\
    \ (first) at second axis\n>>> C.slice(x1, 1, 0, 1).eval({x1: np.asarray([[[[1,2,-3],\n\
    ...                                              [4, 5, 6]]]],dtype=np.float32)})\n\
    array([[[[ 1.],\n         [ 4.]]]], dtype=float32)\n<BLANKLINE>\n>>> # slice using\
    \ constant\n>>> data = np.asarray([[1, 2, -3],\n...                    [4, 5,\
    \  6]], dtype=np.float32)\n>>> x = C.constant(value=data)\n>>> C.slice(x, 0, 1,\
    \ 2).eval()\narray([[ 4.,  5.,  6.]], dtype=float32)\n>>> C.slice(x, 1, 0, 1).eval()\n\
    array([[ 1.],\n       [ 4.]], dtype=float32)\n<BLANKLINE>\n>>> # slice using the\
    \ index overload\n>>> data = np.asarray([[1, 2, -3],\n...                    [4,\
    \ 5,  6]], dtype=np.float32)\n>>> x = C.constant(value=data)\n>>> x[0].eval()\n\
    array([[ 1.,  2.,  -3.]], dtype=float32)\n>>> x[0, [1,2]].eval()\narray([[ 2.,\
    \  -3.]], dtype=float32)\n-[ Example ]-\n#TODO: Make following lines work. Uncomment\
    \ when done #>>> x1[1].eval() #array([[ 4.,  5.,  6.]], dtype=float32) #>>> x1[:,:2,:].eval()\
    \ #array([[ 1.,  2.], #         [ 4.,  5.]], dtype=float32)"
  syntax:
    content: slice()
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.slice
- _type: function
  fullName: cntk.ops.softmax
  langs:
  - python
  module: cntk.ops
  name: softmax
  source:
    id: softmax
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1392
  summary: "Computes the gradient of  at z=``x``. Concretely,\n\nwith the understanding\
    \ that the implementation can use equivalent formulas for efficiency and numerical\
    \ stability.\nThe output is a vector of non-negative numbers that sum to 1 and\
    \ can therefore be interpreted as probabilities for mutually exclusive outcomes\
    \ as in the case of multiclass classification.\nIf `axis` is given, the softmax\
    \ will be computed along that axis.\n-[ Example ]-\n>>> C.softmax([[1, 1, 2, 3]]).eval()\n\
    array([[ 0.082595,  0.082595,  0.224515,  0.610296]], dtype=float32)\n>>> C.softmax([1,\
    \ 1]).eval()\narray([ 0.5,  0.5], dtype=float32)\n>>> C.softmax([[[1, 1], [3,\
    \ 5]]], axis=-1).eval()\narray([[[ 0.5     ,  0.5     ],\n        [ 0.119203,\
    \  0.880797]]], dtype=float32)"
  syntax:
    content: softmax()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: axis along which the softmax operation will be performed
      id: axis
      type:
      - int or Axis
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.softmax
- _type: function
  fullName: cntk.ops.softplus
  langs:
  - python
  module: cntk.ops
  name: softplus
  source:
    id: softplus
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1264
  summary: 'Softplus operation. Computes the element-wise softplus of `x`:

    :math:`     extrm{softplus}(x) = {log(1+exp(x))}`

    The optional `steepness` allows to make the knee sharper (`steepness>1`) or softer,
    by computing `softplus(x * steepness) / steepness`. (For very large steepness,
    this approaches a linear rectifier).

    The output tensor has the same shape as `x`.

    -[ Example ]-

    >>> C.softplus([[-1, -0.5, 0, 1, 2]]).eval()

    array([[ 0.313262,  0.474077,  0.693147,  1.313262,  2.126928]], dtype=float32)

    >>> C.softplus([[-1, -0.5, 0, 1, 2]], steepness=4).eval()

    array([[ 0.004537,  0.031732,  0.173287,  1.004537,  2.000084]], dtype=float32)'
  syntax:
    content: softplus()
    parameters:
    - description: any @cntk.ops.functions.Function that outputs a tensor.
      id: x
      type:
      - numpy.array or Function
    - description: optional steepness factor
      id: steepness
      type:
      - float, optional
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, default to ''
    return:
      description: An instance of @cntk.ops.functions.Function
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.ops.softplus
- _type: function
  fullName: cntk.ops.splice
  langs:
  - python
  module: cntk.ops
  name: splice
  source:
    id: splice
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2018
  summary: "Concatenate the input tensors along an axis.\n-[ Example ]-\n>>> # create\
    \ 2x2 matrix in a sequence of length 1 in a batch of one sample\n>>> data1 = np.asarray([[[1,\
    \ 2],\n...                      [4, 5]]], dtype=np.float32)\n>>> x = C.constant(value=data1)\n\
    >>> # create 3x2 matrix in a sequence of length 1 in a batch of one sample\n>>>\
    \ data2 = np.asarray([[[10, 20],\n...                       [30, 40],\n...   \
    \                    [50, 60]]],dtype=np.float32)\n>>> y = C.constant(value=data2)\n\
    >>> # splice both inputs on axis=0 returns a 5x2 matrix\n>>> C.splice(x, y, axis=1).eval()\n\
    array([[[  1.,   2.],\n        [  4.,   5.],\n        [ 10.,  20.],\n        [\
    \ 30.,  40.],\n        [ 50.,  60.]]], dtype=float32)"
  syntax:
    content: splice()
    parameters:
    - description: one or more input tensors
      id: inputs
    - description: axis along which the concatenation will be performed
      id: axis
      type:
      - int or Axis, optional, keyword only
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional, keyword only
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.splice
- _type: function
  fullName: cntk.ops.sqrt
  langs:
  - python
  module: cntk.ops
  name: sqrt
  source:
    id: sqrt
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1513
  summary: 'Computes the element-wise square-root of `x`:


    -[ Example ]-

    >>> C.sqrt([0., 4.]).eval()

    array([ 0.,  2.], dtype=float32)

    Note: CNTK returns zero for sqrt of negative nubmers, this will be changed to
    retrun NaN'
  syntax:
    content: sqrt()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.sqrt
- _type: function
  fullName: cntk.ops.square
  langs:
  - python
  module: cntk.ops
  name: square
  source:
    id: square
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1539
  summary: 'Computes the element-wise square of `x`:

    -[ Example ]-

    >>> C.square([1., 10.]).eval()

    array([   1.,  100.], dtype=float32)'
  syntax:
    content: square()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.square
- _type: function
  fullName: cntk.ops.stop_gradient
  langs:
  - python
  module: cntk.ops
  name: stop_gradient
  source:
    id: stop_gradient
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 2714
  summary: Outputs its input as it and prevents any gradient contribution from its
    output to its input.
  syntax:
    content: stop_gradient()
    parameters:
    - description: class:*~cntk.ops.functions.Function* that outputs a tensor
      id: input
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.stop_gradient
- _type: function
  fullName: cntk.ops.tanh
  langs:
  - python
  module: cntk.ops
  name: tanh
  source:
    id: tanh
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1325
  summary: "Computes the element-wise tanh of `x`:\nThe output tensor has the same\
    \ shape as `x`.\n-[ Example ]-\n>>> C.tanh([[1,2],[3,4]]).eval()\narray([[ 0.761594,\
    \  0.964028],\n       [ 0.995055,  0.999329]], dtype=float32)"
  syntax:
    content: tanh()
    parameters:
    - description: numpy array or any @cntk.ops.functions.Function that outputs a
        tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.tanh
- _type: function
  fullName: cntk.ops.times
  langs:
  - python
  module: cntk.ops
  name: times
  source:
    id: times
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 876
  summary: "The output of this operation is the matrix product of the two input matrices.\
    \ It supports broadcasting. Sparse is supported in the left operand, if it is\
    \ a matrix. The operator '@' has been overloaded such that in Python 3.5 and later\
    \ X @ W equals times(X, W).\nFor better performance on times operation on sequence\
    \ which is followed by sequence.reduce_sum, use infer_input_rank_to_map=TIMES_REDUCE_SEQUENCE_AXIS_WITHOUT_INFERRED_INPUT_RANK,\
    \ i.e. replace following:\n<!-- literal_block {\"xml:space\": \"preserve\", \"\
    dupnames\": [], \"classes\": [], \"ids\": [], \"backrefs\": [], \"names\": []}\
    \ -->\n\n````\n\n   sequence.reduce_sum(times(seq1, seq2))\n   ````\nwith:\n<!--\
    \ literal_block {\"xml:space\": \"preserve\", \"dupnames\": [], \"classes\": [],\
    \ \"ids\": [], \"backrefs\": [], \"names\": []} -->\n\n````\n\n   times(seq1,\
    \ seq2, infer_input_rank_to_map=TIMES_REDUCE_SEQUENCE_AXIS_WITHOUT_INFERRED_INPUT_RANK)\n\
    \   ````\n-[ Example ]-\n>>> C.times([[1,2],[3,4]], [[5],[6]]).eval()\narray([[\
    \ 17.],\n       [ 39.]], dtype=float32)\n>>> C.times(1.*np.reshape(np.arange(8),\
    \ (2,2,2)),1.*np.reshape(np.arange(8), (2,2,2)), output_rank=1).eval()\narray([[\
    \ 28.,  34.],\n       [ 76.,  98.]])\n>>> C.times(1.*np.reshape(np.arange(8),\
    \ (2,2,2)),1.*np.reshape(np.arange(8), (2,2,2)), output_rank=2).eval()\narray([[[[\
    \  4.,   5.],\n         [  6.,   7.]],\n<BLANKLINE>\n        [[ 12.,  17.],\n\
    \         [ 22.,  27.]]],\n<BLANKLINE>\n<BLANKLINE>\n       [[[ 20.,  29.],\n\
    \         [ 38.,  47.]],\n<BLANKLINE>\n        [[ 28.,  41.],\n         [ 54.,\
    \  67.]]]])"
  syntax:
    content: times()
    parameters:
    - description: left side matrix or tensor
      id: left
    - description: right side matrix or tensor
      id: right
    - description: in case we have tensors as arguments, output_rank represents the
        number of axes to be collapsed in order to transform the tensors into matrices,
        perform the operation and then reshape back (explode the axes)
      id: output_rank
      type:
      - int
    - description: meant for internal use only. Always use default value
      id: infer_input_rank_to_map
      type:
      - '''int'''
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.times
- _type: function
  fullName: cntk.ops.times_transpose
  langs:
  - python
  module: cntk.ops
  name: times_transpose
  source:
    id: times_transpose
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 934
  summary: "The output of this operation is the product of the first (`left`) argument\
    \ with the second (`right`) argument transposed. The second (`right`) argument\
    \ must have a rank of 1 or 2. This operation is conceptually computing `np.dot(left,\
    \ right.T)` except when `right` is a vector in which case the output is `np.dot(left,np.reshape(right,(1,-1)).T)`\
    \ (matching numpy when `left` is a vector).\n-[ Example ]-\n>>> a=np.array([[1,2],[3,4]],dtype=np.float32)\n\
    >>> b=np.array([2,-1],dtype=np.float32)\n>>> c=np.array([[2,-1]],dtype=np.float32)\n\
    >>> d=np.reshape(np.arange(24,dtype=np.float32),(4,3,2))\n>>> print(C.times_transpose(a,\
    \ a).eval())\n[[  5.  11.]\n [ 11.  25.]]\n>>> print(C.times_transpose(a, b).eval())\n\
    [[ 0.]\n [ 2.]]\n>>> print(C.times_transpose(a, c).eval())\n[[ 0.]\n [ 2.]]\n\
    >>> print(C.times_transpose(b, a).eval())\n[ 0.  2.]\n>>> print(C.times_transpose(b,\
    \ b).eval())\n[ 5.]\n>>> print(C.times_transpose(b, c).eval())\n[ 5.]\n>>> print(C.times_transpose(c,\
    \ a).eval())\n[[ 0.  2.]]\n>>> print(C.times_transpose(c, b).eval())\n[[ 5.]]\n\
    >>> print(C.times_transpose(c, c).eval())\n[[ 5.]]\n>>> print(C.times_transpose(d,\
    \ a).eval())\n[[[   2.    4.]\n  [   8.   18.]\n  [  14.   32.]]\n<BLANKLINE>\n\
    \ [[  20.   46.]\n  [  26.   60.]\n  [  32.   74.]]\n<BLANKLINE>\n [[  38.   88.]\n\
    \  [  44.  102.]\n  [  50.  116.]]\n<BLANKLINE>\n [[  56.  130.]\n  [  62.  144.]\n\
    \  [  68.  158.]]]\n>>> print(C.times_transpose(d, b).eval())\n[[[ -1.]\n  [ \
    \ 1.]\n  [  3.]]\n<BLANKLINE>\n [[  5.]\n  [  7.]\n  [  9.]]\n<BLANKLINE>\n [[\
    \ 11.]\n  [ 13.]\n  [ 15.]]\n<BLANKLINE>\n [[ 17.]\n  [ 19.]\n  [ 21.]]]\n>>>\
    \ print(C.times_transpose(d, c).eval())\n[[[ -1.]\n  [  1.]\n  [  3.]]\n<BLANKLINE>\n\
    \ [[  5.]\n  [  7.]\n  [  9.]]\n<BLANKLINE>\n [[ 11.]\n  [ 13.]\n  [ 15.]]\n<BLANKLINE>\n\
    \ [[ 17.]\n  [ 19.]\n  [ 21.]]]"
  syntax:
    content: times_transpose()
    parameters:
    - description: left side tensor
      id: left
    - description: right side matrix or vector
      id: right
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.times_transpose
- _type: function
  fullName: cntk.ops.transpose
  langs:
  - python
  module: cntk.ops
  name: transpose
  source:
    id: transpose
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 1923
  summary: "Swaps two axes of the tensor. The output tensor has the same data but\
    \ with `axis1` and `axis2` swapped.\n-[ Example ]-\n>>> C.transpose([[[0,1],[2,3],[4,5]]],\
    \ 1, 2).eval()\narray([[[ 0.,  2.,  4.],\n        [ 1.,  3.,  5.]]], dtype=float32)"
  syntax:
    content: transpose()
    parameters:
    - description: tensor to be transposed
      id: x
    - description: the axis to swap with `axis2`
      id: axis1
      type:
      - int or Axis
    - description: the axis to swap with `axis1`
      id: axis2
      type:
      - int or Axis
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.transpose
- _type: function
  fullName: cntk.ops.unpooling
  langs:
  - python
  module: cntk.ops
  name: unpooling
  source:
    id: unpooling
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 383
  summary: "Unpools the `operand` using information from `pooling_input`. Unpooling\
    \ mirrors the operations performed by pooling and depends on the values provided\
    \ to the corresponding pooling operation. The output should have the same shape\
    \ as pooling_input. Pooling the result of an unpooling operation should give back\
    \ the original input.\n-[ Example ]-\n>>> img = np.reshape(np.arange(16, dtype\
    \ = np.float32), [1, 4, 4])\n>>> x = C.input_variable(img.shape)\n>>> y = C.pooling(x,\
    \ C.MAX_POOLING, (2,2), (2,2))\n>>> C.unpooling(y, x, C.MAX_UNPOOLING, (2,2),\
    \ (2,2)).eval({x : [img]})\narray([[[[[  0.,   0.,   0.,   0.],\n          [ \
    \ 0.,   5.,   0.,   7.],\n          [  0.,   0.,   0.,   0.],\n          [  0.,\
    \  13.,   0.,  15.]]]]], dtype=float32)"
  syntax:
    content: unpooling()
    parameters:
    - description: unpooling input
      id: operand
    - description: input to the corresponding pooling operation
      id: pooling_input
    - description: only `MAX_UNPOOLING` is supported now
      id: unpooling_type
    - description: dimensions of the unpooling window
      id: unpooling_window_shape
    - description: strides.
      id: strides
      type:
      - default 1
    - description: automatic padding flags for each input dimension.
      id: auto_padding
    - description: precise lower padding for each input dimension
      id: lower_pad
    - description: precise upper padding for each input dimension
      id: upper_pad
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '@cntk.ops.functions.Function'
  type: Method
  uid: cntk.ops.unpooling
references:
- fullName: cntk.ops.abs
  isExternal: false
  name: abs
  parent: cntk.ops
  uid: cntk.ops.abs
- fullName: cntk.ops.alias
  isExternal: false
  name: alias
  parent: cntk.ops
  uid: cntk.ops.alias
- fullName: cntk.ops.argmax
  isExternal: false
  name: argmax
  parent: cntk.ops
  uid: cntk.ops.argmax
- fullName: cntk.ops.argmin
  isExternal: false
  name: argmin
  parent: cntk.ops
  uid: cntk.ops.argmin
- fullName: cntk.ops.as_block
  isExternal: false
  name: as_block
  parent: cntk.ops
  uid: cntk.ops.as_block
- fullName: cntk.ops.as_composite
  isExternal: false
  name: as_composite
  parent: cntk.ops
  uid: cntk.ops.as_composite
- fullName: cntk.ops.associative_multi_arg
  isExternal: false
  name: associative_multi_arg
  parent: cntk.ops
  uid: cntk.ops.associative_multi_arg
- fullName: cntk.ops.batch_normalization
  isExternal: false
  name: batch_normalization
  parent: cntk.ops
  uid: cntk.ops.batch_normalization
- fullName: cntk.ops.ceil
  isExternal: false
  name: ceil
  parent: cntk.ops
  uid: cntk.ops.ceil
- fullName: cntk.ops.clip
  isExternal: false
  name: clip
  parent: cntk.ops
  uid: cntk.ops.clip
- fullName: cntk.ops.combine
  isExternal: false
  name: combine
  parent: cntk.ops
  uid: cntk.ops.combine
- fullName: cntk.ops.constant
  isExternal: false
  name: constant
  parent: cntk.ops
  uid: cntk.ops.constant
- fullName: cntk.ops.convolution
  isExternal: false
  name: convolution
  parent: cntk.ops
  uid: cntk.ops.convolution
- fullName: cntk.ops.convolution_transpose
  isExternal: false
  name: convolution_transpose
  parent: cntk.ops
  uid: cntk.ops.convolution_transpose
- fullName: cntk.ops.cos
  isExternal: false
  name: cos
  parent: cntk.ops
  uid: cntk.ops.cos
- fullName: cntk.ops.dropout
  isExternal: false
  name: dropout
  parent: cntk.ops
  uid: cntk.ops.dropout
- fullName: cntk.ops.element_divide
  isExternal: false
  name: element_divide
  parent: cntk.ops
  uid: cntk.ops.element_divide
- fullName: cntk.ops.element_max
  isExternal: false
  name: element_max
  parent: cntk.ops
  uid: cntk.ops.element_max
- fullName: cntk.ops.element_min
  isExternal: false
  name: element_min
  parent: cntk.ops
  uid: cntk.ops.element_min
- fullName: cntk.ops.element_select
  isExternal: false
  name: element_select
  parent: cntk.ops
  uid: cntk.ops.element_select
- fullName: cntk.ops.element_times
  isExternal: false
  name: element_times
  parent: cntk.ops
  uid: cntk.ops.element_times
- fullName: cntk.ops.elu
  isExternal: false
  name: elu
  parent: cntk.ops
  uid: cntk.ops.elu
- fullName: cntk.ops.equal
  isExternal: false
  name: equal
  parent: cntk.ops
  uid: cntk.ops.equal
- fullName: cntk.ops.exp
  isExternal: false
  name: exp
  parent: cntk.ops
  uid: cntk.ops.exp
- fullName: cntk.ops.floor
  isExternal: false
  name: floor
  parent: cntk.ops
  uid: cntk.ops.floor
- fullName: cntk.ops.forward_backward
  isExternal: false
  name: forward_backward
  parent: cntk.ops
  uid: cntk.ops.forward_backward
- fullName: cntk.ops.future_value
  isExternal: false
  name: future_value
  parent: cntk.ops
  uid: cntk.ops.future_value
- fullName: cntk.ops.greater
  isExternal: false
  name: greater
  parent: cntk.ops
  uid: cntk.ops.greater
- fullName: cntk.ops.greater_equal
  isExternal: false
  name: greater_equal
  parent: cntk.ops
  uid: cntk.ops.greater_equal
- fullName: cntk.ops.hardmax
  isExternal: false
  name: hardmax
  parent: cntk.ops
  uid: cntk.ops.hardmax
- fullName: cntk.ops.input_variable
  isExternal: false
  name: input_variable
  parent: cntk.ops
  uid: cntk.ops.input_variable
- fullName: cntk.ops.labels_to_graph
  isExternal: false
  name: labels_to_graph
  parent: cntk.ops
  uid: cntk.ops.labels_to_graph
- fullName: cntk.ops.leaky_relu
  isExternal: false
  name: leaky_relu
  parent: cntk.ops
  uid: cntk.ops.leaky_relu
- fullName: cntk.ops.less
  isExternal: false
  name: less
  parent: cntk.ops
  uid: cntk.ops.less
- fullName: cntk.ops.less_equal
  isExternal: false
  name: less_equal
  parent: cntk.ops
  uid: cntk.ops.less_equal
- fullName: cntk.ops.log
  isExternal: false
  name: log
  parent: cntk.ops
  uid: cntk.ops.log
- fullName: cntk.ops.log_add_exp
  isExternal: false
  name: log_add_exp
  parent: cntk.ops
  uid: cntk.ops.log_add_exp
- fullName: cntk.ops.minus
  isExternal: false
  name: minus
  parent: cntk.ops
  uid: cntk.ops.minus
- fullName: cntk.ops.negate
  isExternal: false
  name: negate
  parent: cntk.ops
  uid: cntk.ops.negate
- fullName: cntk.ops.not_equal
  isExternal: false
  name: not_equal
  parent: cntk.ops
  uid: cntk.ops.not_equal
- fullName: cntk.ops.optimized_rnnstack
  isExternal: false
  name: optimized_rnnstack
  parent: cntk.ops
  uid: cntk.ops.optimized_rnnstack
- fullName: cntk.ops.output_variable
  isExternal: false
  name: output_variable
  parent: cntk.ops
  uid: cntk.ops.output_variable
- fullName: cntk.ops.param_relu
  isExternal: false
  name: param_relu
  parent: cntk.ops
  uid: cntk.ops.param_relu
- fullName: cntk.ops.parameter
  isExternal: false
  name: parameter
  parent: cntk.ops
  uid: cntk.ops.parameter
- fullName: cntk.ops.past_value
  isExternal: false
  name: past_value
  parent: cntk.ops
  uid: cntk.ops.past_value
- fullName: cntk.ops.per_dim_mean_variance_normalize
  isExternal: false
  name: per_dim_mean_variance_normalize
  parent: cntk.ops
  uid: cntk.ops.per_dim_mean_variance_normalize
- fullName: cntk.ops.placeholder_variable
  isExternal: false
  name: placeholder_variable
  parent: cntk.ops
  uid: cntk.ops.placeholder_variable
- fullName: cntk.ops.plus
  isExternal: false
  name: plus
  parent: cntk.ops
  uid: cntk.ops.plus
- fullName: cntk.ops.pooling
  isExternal: false
  name: pooling
  parent: cntk.ops
  uid: cntk.ops.pooling
- fullName: cntk.ops.random_sample
  isExternal: false
  name: random_sample
  parent: cntk.ops
  uid: cntk.ops.random_sample
- fullName: cntk.ops.random_sample_inclusion_frequency
  isExternal: false
  name: random_sample_inclusion_frequency
  parent: cntk.ops
  uid: cntk.ops.random_sample_inclusion_frequency
- fullName: cntk.ops.reciprocal
  isExternal: false
  name: reciprocal
  parent: cntk.ops
  uid: cntk.ops.reciprocal
- fullName: cntk.ops.reduce_log_sum_exp
  isExternal: false
  name: reduce_log_sum_exp
  parent: cntk.ops
  uid: cntk.ops.reduce_log_sum_exp
- fullName: cntk.ops.reduce_max
  isExternal: false
  name: reduce_max
  parent: cntk.ops
  uid: cntk.ops.reduce_max
- fullName: cntk.ops.reduce_mean
  isExternal: false
  name: reduce_mean
  parent: cntk.ops
  uid: cntk.ops.reduce_mean
- fullName: cntk.ops.reduce_min
  isExternal: false
  name: reduce_min
  parent: cntk.ops
  uid: cntk.ops.reduce_min
- fullName: cntk.ops.reduce_prod
  isExternal: false
  name: reduce_prod
  parent: cntk.ops
  uid: cntk.ops.reduce_prod
- fullName: cntk.ops.reduce_sum
  isExternal: false
  name: reduce_sum
  parent: cntk.ops
  uid: cntk.ops.reduce_sum
- fullName: cntk.ops.relu
  isExternal: false
  name: relu
  parent: cntk.ops
  uid: cntk.ops.relu
- fullName: cntk.ops.reshape
  isExternal: false
  name: reshape
  parent: cntk.ops
  uid: cntk.ops.reshape
- fullName: cntk.ops.roipooling
  isExternal: false
  name: roipooling
  parent: cntk.ops
  uid: cntk.ops.roipooling
- fullName: cntk.ops.round
  isExternal: false
  name: round
  parent: cntk.ops
  uid: cntk.ops.round
- fullName: cntk.ops.sigmoid
  isExternal: false
  name: sigmoid
  parent: cntk.ops
  uid: cntk.ops.sigmoid
- fullName: cntk.ops.sin
  isExternal: false
  name: sin
  parent: cntk.ops
  uid: cntk.ops.sin
- fullName: cntk.ops.slice
  isExternal: false
  name: slice
  parent: cntk.ops
  uid: cntk.ops.slice
- fullName: cntk.ops.softmax
  isExternal: false
  name: softmax
  parent: cntk.ops
  uid: cntk.ops.softmax
- fullName: cntk.ops.softplus
  isExternal: false
  name: softplus
  parent: cntk.ops
  uid: cntk.ops.softplus
- fullName: cntk.ops.splice
  isExternal: false
  name: splice
  parent: cntk.ops
  uid: cntk.ops.splice
- fullName: cntk.ops.sqrt
  isExternal: false
  name: sqrt
  parent: cntk.ops
  uid: cntk.ops.sqrt
- fullName: cntk.ops.square
  isExternal: false
  name: square
  parent: cntk.ops
  uid: cntk.ops.square
- fullName: cntk.ops.stop_gradient
  isExternal: false
  name: stop_gradient
  parent: cntk.ops
  uid: cntk.ops.stop_gradient
- fullName: cntk.ops.tanh
  isExternal: false
  name: tanh
  parent: cntk.ops
  uid: cntk.ops.tanh
- fullName: cntk.ops.times
  isExternal: false
  name: times
  parent: cntk.ops
  uid: cntk.ops.times
- fullName: cntk.ops.times_transpose
  isExternal: false
  name: times_transpose
  parent: cntk.ops
  uid: cntk.ops.times_transpose
- fullName: cntk.ops.transpose
  isExternal: false
  name: transpose
  parent: cntk.ops
  uid: cntk.ops.transpose
- fullName: cntk.ops.unpooling
  isExternal: false
  name: unpooling
  parent: cntk.ops
  uid: cntk.ops.unpooling
