api_name: []
items:
- _type: module
  children:
  - cntk.layers.models.attention.AttentionModel
  fullName: cntk.layers.models.attention
  langs:
  - python
  module: cntk.layers.models.attention
  name: attention
  source:
    id: attention
    path: cntk/layers/models/attention.py
    remote:
      branch: master
      path: cntk/layers/models/attention.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 0
  summary: attention -- standard attention model
  type: Namespace
  uid: cntk.layers.models.attention
- _type: function
  fullName: cntk.layers.models.attention.AttentionModel
  langs:
  - python
  module: cntk.layers.models.attention
  name: AttentionModel
  source:
    id: AttentionModel
    path: cntk/layers/models/attention.py
    remote:
      branch: master
      path: cntk/layers/models/attention.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 18
  summary: Layer factory function to create a function object that implements an attention
    model as described in Bahdanau, et al., "Neural machine translation by jointly
    learning to align and translate."
  syntax:
    parameters:
    - id: attention_dim
    - default: ''
      id: attention_span
    - default: <cntk.default_options.default_override_or object at 0x2afede5c0630>
      id: attention_axis
    - default: <cntk.default_options.default_override_or object at 0x2afede5c05f8>
      id: init
    - default: <cntk.default_options.default_override_or object at 0x2afede5c05c0>
      id: go_backwards
    - default: None
      id: enable_self_stabilization
    - default: None
      id: name
  type: Method
  uid: cntk.layers.models.attention.AttentionModel
references:
- fullName: cntk.layers.models.attention.AttentionModel
  isExternal: false
  name: AttentionModel
  parent: cntk.layers.models.attention
  uid: cntk.layers.models.attention.AttentionModel
