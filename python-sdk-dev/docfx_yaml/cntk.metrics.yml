#YamlMime:PythonReference
api_name: []
items:
- _type: module
  children:
  - cntk.metrics.classification_error
  - cntk.metrics.edit_distance_error
  - cntk.metrics.lambda_rank
  - cntk.metrics.ndcg_at_1
  fullName: cntk.metrics
  langs:
  - python
  module: cntk.metrics
  name: metrics
  source:
    id: metrics
    path: cntk/metrics/__init__.py
    remote:
      branch: master
      path: cntk/metrics/__init__.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 0
  type: Namespace
  uid: cntk.metrics
- _type: function
  fullName: cntk.metrics.classification_error
  langs:
  - python
  module: cntk.metrics
  name: classification_error
  source:
    id: classification_error
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 118
  summary: 'This operation computes the classification error. It finds the index of
    the highest value in the output_vector and compares it to the actual ground truth
    label (the index of the hot bit in the target vector). The result is a scalar
    (i.e., one by one matrix). This is often used as an evaluation criterion. It cannot
    be used as a training criterion though since the gradient is not defined for it.

    -[ Example ]-

    >>> C.classification_error([[1., 2., 3., 4.]], [[0., 0., 0., 1.]]).eval()

    array([[ 0.]], dtype=float32)

    >>> C.classification_error([[1., 2., 3., 4.]], [[0., 0., 1., 0.]]).eval()

    array([[ 1.]], dtype=float32)

    >>> # Note that non-1 values are treated as 0

    >>> C.classification_error([[1., 2., 3., 4.]], [[5., 0., 1., 0.]]).eval()

    array([[ 1.]], dtype=float32)'
  syntax:
    content: classification_error()
    parameters:
    - description: the output values from the network
      id: output_vector
    - description: it is one-hot vector where the hot bit corresponds to the label
        index.
      id: target_vector
    - description: axis along which the classification error will be computed.
      id: axis
      type:
      - int or Axis
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.metrics.classification_error
- _type: function
  fullName: cntk.metrics.edit_distance_error
  langs:
  - python
  module: cntk.metrics
  name: edit_distance_error
  source:
    id: edit_distance_error
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 156
  summary: 'Edit distance error evaluation node with the option of specifying penalty
    of substitution, deletion and insertion, as well as squashing the input sequences
    and ignoring certain samples. Using the classic DP algorithm as described in [https://en.wikipedia.org/wiki/Edit_distance](https://en.wikipedia.org/wiki/Edit_distance),
    adjusted to take into account the penalties.

    Each sequence in the inputs is expected to be a matrix. Prior to computation of
    the edit distance, the operation extracts the indices of maximum element in each
    column. For example, a sequence matrix 1 2 9 1 3 0 3 2 will be represented as
    the vector of labels (indices) as [1, 0, 0, 1], on which edit distance will be
    actually evaluated.

    The node allows to squash sequences of repeating labels and ignore certain labels.
    For example, if squashInputs is true and tokensToIgnore contains label ''-'' then
    given first input sequence as s1="1-12-" and second as s2="-11--122" the edit
    distance will be computed against s1'' = "112" and s2'' = "112".

    The returned error is computed as: EditDistance(s1,s2) * length(s1'') / length(s1)

    Just like ClassificationError and other evaluation nodes, when used as an evaluation
    criterion, the SGD process will aggregate all values over an epoch and report
    the average, i.e. the error rate. Primary objective of this node is for error
    evaluation of CTC training, see formula (1) in "Connectionist Temporal Classification:
    Labelling Unsegmented Sequence Data with Recurrent Neural Networks", [http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_GravesFGS06.pdf](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_GravesFGS06.pdf)

    -[ Example ]-

    i1 = cntk.input_variable(shape=(2,)) i2 = cntk.input_variable(shape=(2,)) arguments
    = {i1 : [[1, 3], [2, 0]], i2 : [[2, 0], [2, 0]]} a = edit_distance_error(i1, i2,
    0, 1, 1, True, [1]) print(a.eval(arguments))'
  syntax:
    content: edit_distance_error()
    parameters:
    - description: first input sequence
      id: input_a
    - description: second input sequence
      id: input_b
    - description: substitution, deletion and insertion penalties
      id: delPen, insPen
      type:
      - subPen,
    - description: whether to merge sequences of identical samples (in both input
        sequences). If true and tokensToIgnore contains label '-' then given first
        input sequence as s1="a-ab-" and second as s2="-aa--abb" the edit distance
        will be computed against s1' = "aab" and s2' = "aab".
      id: squashInputs
    - description: list of samples to ignore during edit distance evaluation (in both
        sequences)
      id: tokensToIgnore
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.metrics.edit_distance_error
- _type: function
  fullName: cntk.metrics.lambda_rank
  langs:
  - python
  module: cntk.metrics
  name: lambda_rank
  source:
    id: lambda_rank
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 16
  summary: "Groups samples according to `group`, sorts them within each group based\
    \ on `output` and computes the Normalized Discounted Cumulative Gain (NDCG) at\
    \ infinity for each group. Concretely, the Discounted Cumulative Gain (DCG) at\
    \ infinity is:\n\nwhere  means the gain of the -th ranked sample.\nThe NDCG is\
    \ just the DCG  divided by the maximum achievable DCG (obtained by placing the\
    \ samples with the largest gain at the top of the ranking).\nSamples in the same\
    \ group must appear in order of decreasing gain.\nIt returns 1 minus the average\
    \ NDCG across all the groups in the minibatch multiplied by 100 times the number\
    \ of samples in the minibatch.\nIn the backward direction it back-propagates LambdaRank\
    \ gradients.\n-[ Example ]-\n>>> group = C.input_variable((1,))\n>>> score = C.input_variable((1,),\
    \ needs_gradient=True)\n>>> gain  = C.input_variable((1,))\n>>> g = np.array([1,\
    \ 1, 2, 2], dtype=np.float32).reshape(4,1,1)\n>>> s = np.array([1, 2, 3, 4], dtype=np.float32).reshape(4,1,1)\n\
    >>> n = np.array([7, 1, 3, 1], dtype=np.float32).reshape(4,1,1)\n>>> f = C.lambda_rank(score,\
    \ gain, group)\n>>> np.round(f.grad({score:s, gain:n, group: g}, wrt=[score]),4)\n\
    array([[[-0.2121]],\n<BLANKLINE>\n       [[ 0.2121]],\n<BLANKLINE>\n       [[-0.1486]],\n\
    <BLANKLINE>\n       [[ 0.1486]]], dtype=float32)"
  syntax:
    content: lambda_rank()
    parameters:
    - description: score of each sample
      id: output
    - description: gain of each sample
      id: gain
    - description: group of each sample
      id: group
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.metrics.lambda_rank
- _type: function
  fullName: cntk.metrics.ndcg_at_1
  langs:
  - python
  module: cntk.metrics
  name: ndcg_at_1
  source:
    id: ndcg_at_1
    path: cntk/internal/swig_helper.py
    remote:
      branch: master
      path: cntk/internal/swig_helper.py
      repo: https://github.com/Microsoft/CNTK
    startLine: 72
  summary: 'Groups samples according to `group`, sorts them within each group based
    on `output` and computes the Normalized Discounted Cumulative Gain (NDCG) at 1
    for each group. Concretely, the NDCG at 1 is:


    where  means the gain of the first ranked sample.

    Samples in the same group must appear in order of decreasing gain.

    It returns the average NDCG at 1 across all the groups in the minibatch multiplied
    by 100 times the number of samples in the minibatch.

    This is a forward-only operation, there is no gradient for it.

    -[ Example ]-

    >>> group = C.input_variable((1,))

    >>> score = C.input_variable((1,))

    >>> gain  = C.input_variable((1,))

    >>> g = np.array([1, 1, 2, 2], dtype=np.float32).reshape(4,1,1)

    >>> s = np.array([2, 1, 3, 1], dtype=np.float32).reshape(4,1,1)

    >>> n = np.array([7, 1, 3, 1], dtype=np.float32).reshape(4,1,1)

    >>> C.ndcg_at_1(score, gain, group).eval({score:s, gain:n, group: g})

    array(400.0, dtype=float32)'
  syntax:
    content: ndcg_at_1()
    parameters:
    - description: score of each sample
      id: output
    - description: gain of each sample
      id: gain
    - description: group of each sample
      id: group
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.metrics.ndcg_at_1
references:
- fullName: cntk.metrics.classification_error
  isExternal: false
  name: classification_error
  parent: cntk.metrics
  uid: cntk.metrics.classification_error
- fullName: cntk.metrics.edit_distance_error
  isExternal: false
  name: edit_distance_error
  parent: cntk.metrics
  uid: cntk.metrics.edit_distance_error
- fullName: cntk.metrics.lambda_rank
  isExternal: false
  name: lambda_rank
  parent: cntk.metrics
  uid: cntk.metrics.lambda_rank
- fullName: cntk.metrics.ndcg_at_1
  isExternal: false
  name: ndcg_at_1
  parent: cntk.metrics
  uid: cntk.metrics.ndcg_at_1
